@article{rogers2024,
  title = {Best Practices for Your Confirmatory Factor Analysis: {{A JASP}} and Lavaan Tutorial},
  shorttitle = {Best Practices for Your Confirmatory Factor Analysis},
  author = {Rogers, Pablo},
  year = {2024},
  month = mar,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02375-7},
  url = {https://link.springer.com/10.3758/s13428-024-02375-7},
  urldate = {2024-03-14},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {ProjetoOS},
  file = {C:\Users\pablo\OneDrive\Documentos\Meus Artigos\Arquivo\Rogers 2024 AFC BRM.pdf}
}


@techreport{aera1999,
  title = {Standards for {{Educational}} and {{Psychological Testing}}},
  author = {{AERA} and {APA} and {NCME}},
  year = {1999},
  address = {Washington},
  institution = {American Educational Research Association, American Psychological Association, \& National Council on Measurement in Education},
  keywords = {artigoCFA}
}

@techreport{aera2014,
  title = {Standards for {{Educational}} and {{Pshychological Testing}}},
  author = {{AERA} and {APA} and {NCME}},
  year = {2014},
  address = {Washington},
  institution = {American Educational Research Association, American Psychological Association \& National Council on Measurement in Education},
  keywords = {artigoCFA}
}

@article{araujo2022,
  title = {Asset Diversification, Financial Well-Being, Quality of Life, and Mental Health: A Study in {{Brazil}}},
  author = {Ara{\'u}jo, Fl{\'a}via Barbosa de Brito and Rogers, Pablo and Peixoto, Fernanda Maciel and Rogers, Dany},
  year = {2022},
  journal = {Revista Contabilidade \& Finan{\c c}as},
  volume = {33},
  number = {90},
  publisher = {FapUNIFESP (SciELO)},
  issn = {1519-7077},
  doi = {10.1590/1808-057x20221470.en},
  abstract = {ABSTRACT This study sought to investigate the relationship between diversification, financial well-being (FWB), quality of life (QoL), and mental health, and to see how FWB mediates this relationship, considering a sample of 1,047 Brazilian investors registered with the Brazilian Securities and Exchange Commission (Comiss{\~a}o de Valores Mobili{\'a}rios [CVM]). In the national and international literature, no studies were found that sought to identify the mediating role of FWB between diversification, QoL, and mental health, as proposed in this study. This research may help brokers and financial institutions, allowing a new look at the profile of investors and their portfolios. It also widens the perspectives on studies of personal finance and mental health in Brazil and around the world. Mediation was conducted through structural equation modeling estimated by robust diagonally weighted least squares (RDWLS). `Asset classes' was adopted as a proxy for diversification. For QoL, the World Health Organization Quality of Life (WHOQOL-100) scale was adopted, while the Beck inventories were used to measure mental health (depression and anxiety). For FWB, the measure of the Brazilian Credit Protection Service (Servi{\c c}o de Prote{\c c}{\~a}o ao Cr{\'e}dito [SPC Brasil]) was used. The results showed a strong relationship between the FWB mediation between the diversification degree (asset classes) and the QoL and mental health scales (anxiety and depression). It was found that the diversification level is related to increased levels of anxiety and depression and decreased QoL in the short term, but when mediated by FWB, it decreases the anxiety and depression levels and increases QoL.RESUMO Este trabalho buscou investigar a rela{\c c}{\~a}o entre diversifica{\c c}{\~a}o, bem-estar financeiro (BEF) e qualidade de vida (QV) e sa{\'u}de mental, e compreender como o BEF medeia essa rela{\c c}{\~a}o, considerando uma amostra de 1.047 investidores brasileiros cadastrados na Comiss{\~a}o de Valores Mobili{\'a}rios (CVM). Na literatura nacional e internacional, n{\~a}o foram encontrados estudos que buscassem identificar o papel mediador do BEF entre a diversifica{\c c}{\~a}o e a QV e a sa{\'u}de mental, como se prop{\~o}e neste estudo. Esta pesquisa pode auxiliar corretoras e institui{\c c}{\~o}es financeiras, possibilitando um novo olhar sobre o perfil dos investidores e suas carteiras. Ainda, amplia as perspectivas sobre os estudos de finan{\c c}as pessoais e sa{\'u}de mental no Brasil e no mundo. A media{\c c}{\~a}o foi realizada por modelagem de equa{\c c}{\~o}es estruturais estimada por m{\'i}nimos quadrados robustos ponderados na diagonal (robust diagonally weighted least squares [RDWLS]). Como proxy de diversifica{\c c}{\~a}o, adotou-se ``classes de ativos''. Para QV, adotou-se a escala World Health Organization Quality of Life (WHOQOL-100), enquanto para mensurar sa{\'u}de mental (depress{\~a}o e ansiedade) usaram-se os invent{\'a}rios de Beck. Para BEF, utilizou-se a medida do Servi{\c c}o de Prote{\c c}{\~a}o ao Cr{\'e}dito (SPC) do Brasil. Os resultados apontaram forte rela{\c c}{\~a}o de media{\c c}{\~a}o do BEF entre o grau de diversifica{\c c}{\~a}o (classes de ativos) e as escalas de QV e sa{\'u}de mental (ansiedade e depress{\~a}o). Constatou-se que o n{\'i}vel de diversifica{\c c}{\~a}o est{\'a} relacionado com o aumento dos n{\'i}veis de ansiedade e depress{\~a}o e com a redu{\c c}{\~a}o da QV no curto prazo, mas, quando mediado pelo BEF, reduz os n{\'i}veis de ansiedade e depress{\~a}o e aumenta a QV.},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Ara√∫jo et al_2022_Asset diversification, financial well-being, quality of life, and mental health.pdf}
}

@misc{arbuckle2019,
  title = {Amos},
  author = {Arbuckle, J. L.},
  year = {2019},
  address = {Chicago},
  howpublished = {IBM Corp},
  keywords = {artigoCFA}
}

@article{bandalos2014,
  title = {Relative {{Performance}} of {{Categorical Diagonally Weighted Least Squares}} and {{Robust Maximum Likelihood Estimation}}},
  author = {Bandalos, Deborah L.},
  year = {2014},
  month = jan,
  journal = {Structural Equation Modeling},
  volume = {21},
  number = {1},
  pages = {102--116},
  issn = {10705511},
  doi = {10.1080/10705511.2014.859510},
  abstract = {Robust maximum likelihood (ML) and categorical diagonally weighted least squares (cat-DWLS) estimation have both been proposed for use with categorized and nonnormally distributed data. This study compares results from the 2 methods in terms of parameter estimate and standard error bias, power, and Type I error control, with unadjusted ML and WLS estimation methods included for purposes of comparison. Conditions manipulated include model misspecification, level of asymmetry, level and categorization, sample size, and type and size of the model. Results indicate that cat-DWLS estimation method results in the least parameter estimate and standard error bias under the majority of conditions studied. Cat-DWLS parameter estimates and standard errors were generally the least affected by model misspecification of the estimation methods studied. Robust ML also performed well, yielding relatively unbiased parameter estimates and standard errors. However, both cat-DWLS and robust ML resulted in low power under conditions of high data asymmetry, small sample sizes, and mild model misspecification. For more optimal conditions, power for these estimators was adequate. {\copyright} 2014 Copyright Taylor and Francis Group, LLC.},
  keywords = {,artigoCFA,categorical data,DWLS,Estimator,Evidencia,ML,Para LER,RML,Robust estimator,Robust Maximum Likelihood,WLS},
  file = {C:\Users\pablo\OneDrive\Zotero\Bandalos_2014_Relative Performance of Categorical Diagonally Weighted Least Squares and.pdf}
}

@book{bandalos2018,
  title = {Measurement Theory and Applications for the Social Sciences},
  author = {Bandalos, Deborah L.},
  year = {2018},
  publisher = {Guilford Press},
  address = {New York},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Books\Psychometrics\Main\Measurement Theory and Applications for the Social Sciences - Bandalos (2018).pdf}
}

@article{bell2023,
  title = {The {{Impact}} of {{Measurement Model Misspecification}} on {{Coefficient Omega Estimates}} of {{Composite Reliability}}},
  author = {Bell, Stephanie M. and Chalmers, R. Philip and Flora, David B.},
  year = {2023},
  journal = {Educational and Psychological Measurement},
  pages = {1--36},
  publisher = {SAGE Publications Inc.},
  issn = {15523888},
  doi = {10.1177/00131644231155804},
  abstract = {Coefficient omega indices are model-based composite reliability estimates that have become increasingly popular. A coefficient omega index estimates how reliably an observed composite score measures a target construct as represented by a factor in a factor-analysis model; as such, the accuracy of omega estimates is likely to depend on correct model specification. The current paper presents a simulation study to investigate the performance of omega-unidimensional (based on the parameters of a one-factor model) and omega-hierarchical (based on a bifactor model) under correct and incorrect model misspecification for high and low reliability composites and different scale lengths. Our results show that coefficient omega estimates are unbiased when calculated from the parameter estimates of a properly specified model. However, omega-unidimensional produced positively biased estimates when the population model was characterized by unmodeled error correlations or multidimensionality, whereas omega-hierarchical was only slightly biased when the population model was either a one-factor model with correlated errors or a higher-order model. These biases were higher when population reliability was lower and increased with scale length. Researchers should carefully evaluate the feasibility of a one-factor model before estimating and reporting omega-unidimensional.},
  keywords = {,artigoCFA,coefficient omega,factor reliability,item factor analysis,Lido,measurement model,Omega,omega-hierarchical,OneNote,Reliability},
  file = {C:\Users\pablo\OneDrive\Zotero\Bell et al_2023_The Impact of Measurement Model Misspecification on Coefficient Omega Estimates.pdf}
}

@misc{bentler2020,
  title = {{{EQS}} 6.4 for {{Windows}}},
  author = {Bentler, Peter M. and Wu, Erik},
  year = {2020},
  url = {https://mvsoft.com},
  urldate = {2023-02-22},
  howpublished = {Multivariate Software, Inc},
  keywords = {artigoCFA}
}

@book{brown2015,
  title = {Confirmatory {{Factor Analysis}} for {{Applied Research}}},
  author = {Brown, Timothy A.},
  year = {2015},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,CFA,factor analysis,Geral,Lido,structural equation modeling},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Brown (2015) - Confirmatory Factor Analysis for Applied Research.pdf}
}

@incollection{brown2023,
  title = {Confirmatory {{Factor Analysis}}},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {Brown, Timothy A.},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@book{byrne2016,
  title = {Structural {{Equation Modeling}} with {{AMOS}}: {{Basic Concepts}}, {{Application}}, and {{Programming}}},
  author = {Byrne, Barbara M.},
  year = {2016},
  publisher = {Routledge Taylor \& Francis Group},
  address = {New York},
  keywords = {AMOS,artigoCFA,Geral,Lido,SEM,structural equation modeling},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\AMOS\Structural Equation Modeling with Amos - Byrne (2016).pdf}
}

@article{cho2022,
  title = {Reliability and {{Omega Hierarchical}} in {{Multidimensional Data}}: {{A Comparison}} of {{Various Estimators}}},
  author = {Cho, Eunseong},
  year = {2022},
  journal = {Psychological Methods},
  publisher = {American Psychological Association},
  issn = {1082989X},
  doi = {10.1037/met0000525},
  abstract = {The current guidelines for estimating reliability recommend using two omega combinations in multidimensional data. One omega is for factor analysis (FA) reliability estimators, and the other omega is for omega hierarchical estimators (i.e., {$\omega$}h). This study challenges these guidelines. Specifically, the following three questions are asked: (a) Do FA reliability estimators outperform non-FA reliability estimators? (b) Is it always desirable to estimate {$\omega$}h? (c) What are the best reliability and {$\omega$}h estimators? This study addresses these issues through a Monte Carlo simulation of reliability and {$\omega$}h estimators. The conclusions are given as follows. First, the performance differences among most reliability estimators are small, and the performance of FA estimators is comparable to that of non-FA estimators. However, the current, most-recommended estimators, that is, estimators based on the bifactor model and exploratory factor analysis, tend to overestimate reliability. Second, the accuracy of {$\omega$}h estimators is much lower than that of reliability estimators, so we should perform {$\omega$}h estimation selectively only on data that meet several requirements. Third, exploratory bifactor analysis is more accurate than confirmatory bifactor analysis only in the presence of cross-loading; otherwise, exploratory bifactor analysis is less accurate than confirmatory bifactor analysis. Fourth, techniques known to improve the Schmid-Leiman (SL) transformation are not superior to SL transformation but have different advantages. This study provides an R Shiny app that allows users to obtain multidimensional reliability and {$\omega$}h estimates with a few mouse clicks.},
  keywords = {,artigoCFA,Coefficient omega,Evidencia,Exploratory bifactor model,Lido,Monte carlo simulation,Omega,Omega hierarchical,OneNote,Reliability,Schmid-leiman transformation},
  file = {C:\Users\pablo\OneDrive\Zotero\Cho_2022_Reliability and Omega Hierarchical in Multidimensional Data.pdf}
}

@book{cohen2022,
  title = {Psychological {{Testing}} and {{Assessment}}: {{An Introduction}} to {{Test}} and {{Measurement}}},
  author = {Cohen, Ronald Jay and Schneider, Joel W. and Tobin, Ren{\'e}e M.},
  year = {2022},
  publisher = {McGraw Hill LLC},
  address = {New York},
  keywords = {artigoCFA}
}

@book{collier2020,
  title = {Applied {{Structural Equation Modeling Using AMOS}}: {{Basic}} to {{Advanced Techniques}}},
  author = {Collier, Joel E.},
  year = {2020},
  publisher = {Routledge},
  address = {New York},
  keywords = {AMOS,artigoCFA,Geral,SEM},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\AMOS\Applied Structural Equation Modeling using AMOS - Collier (2020).pdf}
}

@article{crede2019,
  title = {Questionable Research Practices When Using Confirmatory Factor Analysis},
  author = {Crede, Marcus and Harms, Peter},
  year = {2019},
  month = feb,
  journal = {Journal of Managerial Psychology},
  volume = {34},
  number = {1},
  pages = {18--30},
  publisher = {Emerald Group Publishing Ltd.},
  issn = {02683946},
  doi = {10.1108/JMP-06-2018-0272},
  urldate = {2021-03-22},
  abstract = {Purpose: The purpose of this paper is to describe common questionable research practices (QRPs) engaged in by management researchers who use confirmatory factor analysis (CFA) as part of their analysis. Design/methodology/approach: The authors describe seven questionable analytic practices and then review one year of journal articles published in three top-tier management journals to estimate the base rate of these practices. Findings: The authors find that CFA analyses are characterized by a high base rate of QRPs with one practice occurring for over 90 percent of all assessed articles. Research limitations/implications: The findings of this paper call into question the validity and trustworthiness of results reported in much of the management literature. Practical implications: The authors provide tentative guidelines of how editors and reviewers might reduce the degree to which the management literature is characterized by these QRPs. Originality/value: This is the first paper to estimate the base rate of six QRPs relating to the widely used analytic tool referred to as CFA in the management literature.},
  keywords = {artigoCFA,CFA,Importante,Lido,Psychometrics,Research methods,Scale development,Structural equation modelling},
  file = {C:\Users\pablo\OneDrive\Zotero\Crede_Harms_2019_Questionable research practices when using confirmatory factor analysis.pdf}
}

@article{davvetas2020,
  title = {Ten Basic Questions about Structural Equations Modeling You Should Know the Answers to -- {{But}} Perhaps You Don't},
  author = {Davvetas, Vasileios and Diamantopoulos, Adamantios and Zaefarian, Ghasem and Sichtmann, Christina},
  year = {2020},
  month = oct,
  journal = {Industrial Marketing Management},
  volume = {90},
  pages = {252--263},
  publisher = {Elsevier Inc.},
  issn = {00198501},
  doi = {10.1016/j.indmarman.2020.07.016},
  abstract = {Structural Equations Modeling (SEM) has enjoyed increased popularity as an analytical method among Industrial Marketing Management (IMM) authors over the last years. Despite such popularity, many authors fail to understand the basic principles of the method and reviewers are frequently confronted with manuscripts suffering from erroneous applications, insufficient reporting and questionable interpretation of SEM-based findings. Addressing this issue, the present article presents -- in non-technical language -- the most basic concepts related to SEM, resolves common misconceptions about the method's application and provides hands-on advice to IMM authors and reviewers dealing with SEM-based manuscripts. Structured along ten fundamental questions, the article covers issues related to (1) latent variables and their scaling, (2) types of parameters in SEM, (3) unstandardized and standardized estimates, (4) model identification, (5) model constraints, (6) model fit, (7) independence and saturated models, (8) modification indices, (9) nested models, and (10) equivalent models. After illustrating these concepts with the use of examples, the article concludes with a list of guidelines addressed both to IMM authors crafting manuscripts using SEM and the peers reviewing them.},
  keywords = {,artigoCFA,Confirmatory factor analysis,Geral,Importante,Lido,SEM,Structural equations modeling,Survey research},
  file = {C:\Users\pablo\OneDrive\Zotero\Davvetas et al_2020_Ten basic questions about structural equations modeling you should know the.pdf}
}

@article{dunn2014,
  title = {From Alpha to Omega: {{A}} Practical Solution to the Pervasive Problem of Internal Consistency Estimation},
  author = {Dunn, Thomas J. and Baguley, Thom and Brunsden, Vivienne},
  year = {2014},
  journal = {British Journal of Psychology},
  volume = {105},
  number = {3},
  pages = {399--412},
  publisher = {{John Wiley and Sons Ltd.}},
  issn = {20448295},
  doi = {10.1111/bjop.12046},
  abstract = {Coefficient alpha is the most popular measure of reliability (and certainly of internal consistency reliability) reported in psychological research. This is noteworthy given the numerous deficiencies of coefficient alpha documented in the psychometric literature. This mismatch between theory and practice appears to arise partly because users of psychological scales are unfamiliar with the psychometric literature on coefficient alpha and partly because alternatives to alpha are not widely known. We present a brief review of the psychometric literature on coefficient alpha, followed by a practical alternative in the form of coefficient omega. To facilitate the shift from alpha to omega, we also present a brief guide to the calculation of point and interval estimates of omega using a free, open source software environment. {\copyright} 2013 The British Psychological Society.},
  pmid = {24844115},
  keywords = {Alpha,artigoCFA,Omega,Para LER,R software},
  file = {C:\Users\pablo\OneDrive\Zotero\Dunn et al_2014_From alpha to omega.pdf}
}

@incollection{enders2023,
  title = {Fitting Structural {{Equation Models}} with {{Missing}} Data},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {Enders, Craig},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@incollection{feng2023,
  title = {Power {{Analysis}} within a {{Structural Equation Modeling Framework}}},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {Feng, Yi and Hancock, Gregory R.},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral,Importante,Lido,OneNote,Power analysis,SEM},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@article{flake2017,
  title = {Construct {{Validation}} in {{Social}} and {{Personality Research}}: {{Current Practices}} and {{Recommendations}}},
  author = {Flake, Jessica K. and Pek, Jolynn and Hehman, Eric},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {370--378},
  issn = {1948-5506},
  doi = {10.1177/1948550617693063},
  url = {http://journals.sagepub.com/doi/10.1177/1948550617693063},
  abstract = {{$<$}p{$>$}The verity of results about a psychological construct hinges on the validity of its measurement, making construct validation a fundamental methodology to the scientific process. We reviewed a representative sample of articles published in the Journal of Personality and Social Psychology for construct validity evidence. We report that latent variable measurement, in which responses to items are used to represent a construct, is pervasive in social and personality research. However, the field does not appear to be engaged in best practices for ongoing construct validation. We found that validity evidence of existing and author-developed scales was lacking, with coefficient {$\alpha$} often being the only psychometric evidence reported. We provide a discussion of why the construct validation framework is important for social and personality researchers and recommendations for improving practice.{$<$}/p{$>$}},
  keywords = {,artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Flake et al_2017_Construct Validation in Social and Personality Research.pdf}
}

@article{flake2020,
  title = {Measurement {{Schmeasurement}}: {{Questionable Measurement Practices}} and {{How}} to {{Avoid Them}}},
  author = {Flake, Jessica Kay and Fried, Eiko I.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {456--465},
  issn = {2515-2459},
  doi = {10.1177/2515245920952393},
  abstract = {{$<$}p{$>$}In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study's inferences, and are necessary for meaningful replication studies.{$<$}/p{$>$}},
  keywords = {,artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Flake_Fried_2020_Measurement Schmeasurement.pdf}
}

@article{flake2022,
  title = {Construct Validity and the Validity of Replication Studies: {{A}} Systematic Review.},
  author = {Flake, Jessica Kay and Davidson, Ian J. and Wong, Octavia and Pek, Jolynn},
  year = {2022},
  month = may,
  journal = {American Psychologist},
  volume = {77},
  number = {4},
  pages = {576--588},
  issn = {1935-990X},
  doi = {10.1037/amp0001006},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Flake et al_2022_Construct validity and the validity of replication studies.pdf}
}

@article{flora2012,
  title = {Old and {{New Ideas}} for {{Data Screening}} and {{Assumption Testing}} for {{Exploratory}} and {{Confirmatory Factor Analysis}}},
  author = {Flora, David B. and LaBrish, Cathy and Chalmers, R. Philip},
  year = {2012},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {3},
  number = {MAR},
  pages = {55},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00055},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00055/abstract},
  urldate = {2021-03-22},
  abstract = {We provide a basic review of the data screening and assumption testing issues relevant to exploratory and confirmatory factor analysis along with practical advice for conducting analyses that are sensitive to these concerns. Historically, factor analysis was developed for explaining the relationships among many continuous test scores, which led to the expression of the common factor model as a multivariate linear regression model with observed, continuous variables serving as dependent variables, and unobserved factors as the independent, explanatory variables. Thus, we begin our paper with a review of the assumptions for the common factor model and data screening issues as they pertain to the factor analysis of continuous observed variables. In particular, we describe how prin- ciples from regression diagnostics also apply to factor analysis. Next, because modern applications of factor analysis frequently involve the analysis of the individual items from a single test or questionnaire, an important focus of this paper is the factor analysis of items. Although the traditional linear factor model is well-suited to the analysis of con- tinuously distributed variables, commonly used item types, including Likert-type items, almost always produce dichotomous or ordered categorical variables. We describe how relationships among such items are often not well described by product-moment correla- tions, which has clear ramifications for the traditional linear factor analysis. An alternative, non-linear factor analysis using polychoric correlations has become more readily available to applied researchers and thus more popular. Consequently, we also review the assumptions and data-screening issues involved in this method.Throughout the paper, we demonstrate these procedures using an historic data set of nine cognitive ability variables. {\copyright} 2012 Flora, LaBrish and Chalmers.},
  keywords = {artigoCFA,CFA,CMB,Confirmatory factor analysis,Data cleaning,Data Screening,EFA,Exploratory factor analysis,Geral,Importante,Item factor analysis,Para LER,Structural equation},
  file = {C:\Users\pablo\OneDrive\Zotero\Flora et al_2012_Old and New Ideas for Data Screening and Assumption Testing for Exploratory and.pdf}
}

@article{flora2017,
  title = {The Purpose and Practice of Exploratory and Confirmatory Factor Analysis in Psychological Research: {{Decisions}} for Scale Development and Validation},
  author = {Flora, David B. and Flake, Jessica K.},
  year = {2017},
  month = apr,
  journal = {Canadian Journal of Behavioural Science},
  volume = {49},
  number = {2},
  pages = {78--88},
  publisher = {American Psychological Association Inc.},
  issn = {18792669},
  doi = {10.1037/cbs0000069},
  abstract = {There are many high-quality resources available which describe best practices in the implementation of both exploratory factor analysis (EFA) and confirmatory factor analysis (CFA). Yet, partly owing to the complexity of these procedures, confusion persists among psychologists with respect to the implementation of EFA and CFA. Primary among these misunderstandings is the very mathematical distinction between EFA and CFA. The current paper uses a brief example to illustrate the difference between the statistical models underlying EFA and CFA, both of which are particular instantiations of the more general common factor model. Next, important considerations for the implementation of EFA and CFA discussed in this paper include the need to account for the categorical nature of item-level observed variables in factor analyses, the use of factor analysis in studies of the psychometric properties of new tests or questionnaires and previously developed tests, decisions about whether to use EFA or CFA in these contexts, and the importance of replication of factor analytic models in the ongoing pursuit of validation.},
  keywords = {,artigoCFA,best practices,confirmatory factor analysis,exploratory factor analysis,psychometric properties,validity},
  file = {C:\Users\pablo\OneDrive\Zotero\Flora_Flake_2017_The purpose and practice of exploratory and confirmatory factor analysis in.pdf}
}

@article{flora2020,
  title = {Your {{Coefficient Alpha Is Probably Wrong}}, but {{Which Coefficient Omega Is Right}}? {{A Tutorial}} on {{Using R}} to {{Obtain Better Reliability Estimates}}},
  author = {Flora, David B.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {484--501},
  publisher = {SAGE Publications Inc.},
  issn = {2515-2459},
  doi = {10.1177/2515245920951747},
  url = {http://journals.sagepub.com/doi/10.1177/2515245920951747},
  abstract = {{$<$}p{$>$}Measurement quality has recently been highlighted as an important concern for advancing a cumulative psychological science. An implication is that researchers should move beyond mechanistically reporting coefficient alpha toward more carefully assessing the internal structure and reliability of multi-item scales. Yet a researcher may be discouraged upon discovering that a prominent alternative to alpha, namely, coefficient omega, can be calculated in a variety of ways. In this Tutorial, I alleviate this potential confusion by describing alternative forms of omega and providing guidelines for choosing an appropriate omega estimate pertaining to the measurement of a target construct represented with a confirmatory factor analysis model. Several applied examples demonstrate how to compute different forms of omega in R.{$<$}/p{$>$}},
  keywords = {,alpha,artigoCFA,assessment,confirmatory factor analysis,Importante,Lido,measurement,Omega,OneNote,open data,open materials,psychometrics,R,R software,Reliability},
  file = {C:\Users\pablo\OneDrive\Zotero\Flora_2020_Your Coefficient Alpha Is Probably Wrong, but Which Coefficient Omega Is Right.pdf}
}

@misc{fox2022,
  title = {Sem: {{Structural Equation Modeling}}},
  author = {Fox, John},
  year = {2022},
  url = {https://cran.r-project.org/web/packages/sem/},
  urldate = {2023-10-20},
  howpublished = {R package},
  keywords = {artigoCFA}
}

@book{furr2021,
  title = {Psychometrics: {{An Introduction}}},
  author = {Furr, Michael R},
  year = {2021},
  publisher = {SAGE Publications},
  keywords = {artigoCFA}
}

@article{gilroy2019,
  title = {Furthering {{Open Science}} in {{Behavior Analysis}}: {{An Introduction}} and {{Tutorial}} for {{Using GitHub}} in {{Research}}},
  shorttitle = {Furthering {{Open Science}} in {{Behavior Analysis}}},
  author = {Gilroy, Shawn P. and Kaplan, Brent A.},
  year = {2019},
  month = sep,
  journal = {Perspectives on Behavior Science},
  volume = {42},
  number = {3},
  pages = {565--581},
  issn = {2520-8969, 2520-8977},
  doi = {10.1007/s40614-019-00202-5},
  url = {http://link.springer.com/10.1007/s40614-019-00202-5},
  urldate = {2024-01-30},
  langid = {english},
  keywords = {artigoCFA,Lido,OneNote,Version Control},
  file = {C:\Users\pablo\OneDrive\Zotero\Gilroy_Kaplan_2019_Furthering Open Science in Behavior Analysis3.pdf}
}

@article{goodboy2020,
  title = {Omega over Alpha for Reliability Estimation of Unidimensional Communication Measures},
  author = {Goodboy, Alan K. and Martin, Matthew M.},
  year = {2020},
  month = oct,
  journal = {Annals of the International Communication Association},
  volume = {44},
  number = {4},
  pages = {422--439},
  publisher = {Routledge},
  issn = {23808977},
  doi = {10.1080/23808985.2020.1846135},
  abstract = {Cronbach's alpha (coefficient {$\alpha$}) is the conventional statistic communication scholars use to estimate the reliability of multi-item measurement instruments. For many, if not most communication measures, {$\alpha$} should not be calculated for reliability estimation. Instead, coefficient omega ({$\omega$}) should be reported as it aligns with the definition of reliability itself. In this primer, we review {$\alpha$} and {$\omega$}, and explain why {$\omega$} should be the new `gold standard' in reliability estimation. Using Mplus, we demonstrate how {$\omega$} is calculated on an available data set and show how preliminary scales can be revised with `{$\omega$} if item deleted.' We also list several easy-to-use resources to calculate {$\omega$} in other software programs. Communication researchers should routinely report {$\omega$} instead of {$\alpha$}.},
  keywords = {,Alpha,artigoCFA,Communication measurement,Cronbach's alpha,Geral,Lido,McDonald's omega,Mplus,Omega,OneNote,Reliability},
  file = {C:\Users\pablo\OneDrive\Zotero\Goodboy_Martin_2020_Omega over alpha for reliability estimation of unidimensional communication.pdf}
}

@article{green1997,
  title = {Effect of the Number of Scale Points on Chi-square Fit Indices in Confirmatory Factor Analysis},
  author = {Green, Samuel B. and Akey, Theresa M. and Fleming, Kandace K. and Hershberger, Scott L. and Marquis, Janet G.},
  year = {1997},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {4},
  number = {2},
  pages = {108--120},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705519709540064},
  url = {http://www.tandfonline.com/doi/abs/10.1080/10705519709540064},
  urldate = {2024-02-01},
  langid = {english},
  keywords = {artigoCFA,CFA,Indices,Likert,Ordinal data},
  file = {C:\Users\pablo\OneDrive\Zotero\Green et al_1997_Effect of the number of scale points on chi‚Äêsquare fit indices in confirmatory.pdf}
}

@article{green2015,
  title = {Evaluation of {{Dimensionality}} in the {{Assessment}} of {{Internal Consistency Reliability}}: {{Coefficient Alpha}} and {{Omega Coefficients}}},
  author = {Green, Samuel B. and Yang, Yanyun},
  year = {2015},
  month = dec,
  journal = {Educational Measurement: Issues and Practice},
  volume = {34},
  number = {4},
  pages = {14--20},
  issn = {17453992},
  doi = {10.1111/emip.12100},
  abstract = {In the lead article, Davenport, Davison, Liou, \& Love demonstrate the relationship among homogeneity, internal consistency, and coefficient alpha, and also distinguish among them. These distinctions are important because too often coefficient alpha-a reliability coefficient-is interpreted as an index of homogeneity or internal consistency. We argue that factor analysis should be conducted before calculating internal consistency estimates of reliability. If factor analysis indicates the assumptions underlying coefficient alpha are met, then it can be reported as a reliability coefficient. However, to the extent that items are multidimensional, alternative internal consistency reliability coefficients should be computed based on the parameter estimates of the factor model. Assuming a bifactor model evidenced good fit, and the measure was designed to assess a single construct, omega hierarchical-the proportion of variance of the total scores due to the general factor-should be presented. Omega-the proportion of variance of the total scores due to all factors-also should be reported in that it represents a more traditional view of reliability, although it is computed within a factor analytic framework. By presenting both these coefficients and potentially other omega coefficients, the reliability results are less likely to be misinterpreted.},
  keywords = {artigoCFA,Coefficient,Internal consistency,Lido,Omega,Reliability,Unidimensionality,Validity},
  file = {C:\Users\pablo\OneDrive\Zotero\Green_Yang_2015_Evaluation of Dimensionality in the Assessment of Internal Consistency.pdf}
}

@article{grewal2004,
  title = {Multicollinearity and {{Measurement Error}} in {{Structural Equation Models}}: {{Implications}} for {{Theory Testing}}},
  shorttitle = {Multicollinearity and {{Measurement Error}} in {{Structural Equation Models}}},
  author = {Grewal, Rajdeep and Cote, Joseph A. and Baumgartner, Hans},
  year = {2004},
  month = nov,
  journal = {Marketing Science},
  volume = {23},
  number = {4},
  pages = {519--529},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.1040.0070},
  url = {https://pubsonline.informs.org/doi/10.1287/mksc.1040.0070},
  urldate = {2024-02-07},
  abstract = {The literature on structural equation models is unclear on whether and when multicollinearity may pose problems in theory testing (Type II errors). Two Monte Carlo simulation experiments show that multicollinearity can cause problems under certain conditions, specifically: (1) when multicollinearity is extreme, Type II error rates are generally unacceptably high (over 80\%), (2) when multicollinearity is between 0.6 and 0.8, Type II error rates can be substantial (greater than 50\% and frequently above 80\%) if composite reliability is weak, explained variance (R               2               ) is low, and sample size is relatively small. However, as reliability improves (0.80 or higher), explained variance R               2               reaches 0.75, and sample becomes relatively large, Type II error rates become negligible. (3) When multicollinearity is between 0.4 and 0.5, Type II error rates tend to be quite small, except when reliability is weak, R               2               is low, and sample size is small, in which case error rates can still be high (greater than 50\%). Methods for detecting and correcting multicollinearity are briefly discussed. However, since multicollinearity is difficult to manage after the fact, researchers should avoid problems by carefully managing the factors known to mitigate multicollinearity problems (particularly measurement error).},
  langid = {english},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Grewal et al_2004_Multicollinearity and Measurement Error in Structural Equation Models.pdf}
}

@article{groskurth2023,
  title = {Why We Need to Abandon Fixed Cutoffs for Goodness-of-Fit Indices: {{An}} Extensive Simulation and Possible Solutions},
  author = {Groskurth, Katharina and Bluemke, Matthias and Lechner, Clemens M.},
  year = {2023},
  month = aug,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02193-3},
  url = {https://link.springer.com/10.3758/s13428-023-02193-3},
  abstract = {{$<$}p{$>$} To evaluate model fit in confirmatory factor analysis, researchers compare goodness-of-fit indices (GOFs) against fixed cutoff values (e.g., CFI \&gt; .950) derived from simulation studies. Methodologists have cautioned that cutoffs for GOFs are only valid for settings similar to the simulation scenarios from which cutoffs originated. Despite these warnings, fixed cutoffs for popular GOFs (i.e., {$\chi$} \textsuperscript{2} , {$\chi$} \textsuperscript{2} / {$<$}italic{$>$}df{$<$}/italic{$>$} , CFI, RMSEA, SRMR) continue to be widely used in applied research. We (1) argue that the practice of using fixed cutoffs needs to be abandoned and (2) review time-honored and emerging alternatives to fixed cutoffs. We first present the most in-depth simulation study to date on the sensitivity of GOFs to model misspecification (i.e., misspecified factor dimensionality and unmodeled cross-loadings) and their susceptibility to further data and analysis characteristics (i.e., estimator, number of indicators, number and distribution of response options, loading magnitude, sample size, and factor correlation). We included all characteristics identified as influential in previous studies. Our simulation enabled us to replicate well-known influences on GOFs and establish hitherto unknown or underappreciated ones. In particular, the magnitude of the factor correlation turned out to moderate the effects of several characteristics on GOFs. Second, to address these problems, we discuss several strategies for assessing model fit that take the dependency of GOFs on the modeling context into account. We highlight tailored (or ``dynamic'') cutoffs as a way forward. We provide convenient tables with scenario-specific cutoffs as well as regression formulae to predict cutoffs tailored to the empirical setting of interest. {$<$}/p{$>$}},
  keywords = {artigoCFA,CFA,Evidencia,Importante,Indices,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\Groskurth et al_2023_Why we need to abandon fixed cutoffs for goodness-of-fit indices.pdf}
}

@book{hair2017,
  title = {Advanced {{Issues}} in {{Partial Least Squares Structural Equation Modeling}}},
  author = {Hair, Joseph F. and Sarstedt, Marko and Ringle, Christian and Gudergan, Siegfried P.},
  year = {2017},
  publisher = {SAGE Publications, Inc},
  address = {London},
  keywords = {artigoCFA}
}

@book{hair2019book,
  title = {Multivariate {{Data Analysis}}},
  author = {Hair, Joseph and Black, William and Babin, Barry and Anderson, Rolph},
  year = {2019},
  publisher = {Cengage Learning},
  address = {Hampshire},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hair et al (2019) - Multivariate Data Analysis.pdf}
}

@book{hair2022,
  title = {A {{Primer}} on {{Partial Least Squares Structural Equation Modeling}} ({{PLS-SEM}})},
  author = {Hair, Joseph F. and Hult, Tomas M. G. and Ringle, Christian M. and Sarstedt, Marko},
  year = {2022},
  publisher = {Sage Publications},
  address = {Thousand Oaks},
  keywords = {artigoCFA}
}

@book{harrington2009,
  title = {Confirmatory {{Factor Analysis}}},
  author = {Harrington, Donna},
  year = {2009},
  publisher = {Oxford University Press},
  address = {New York},
  keywords = {artigoCFA,CFA,Geral},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Basic\Harrington (2009) - CFA Pocktet Book.pdf}
}

@article{hayes2020,
  title = {Use {{Omega Rather}} than {{Cronbach}}'s {{Alpha}} for {{Estimating Reliability}}. {{But}}{\dots}},
  author = {Hayes, Andrew F. and Coutts, Jacob J.},
  year = {2020},
  month = jan,
  journal = {Communication Methods and Measures},
  volume = {14},
  number = {1},
  pages = {1--24},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312458.2020.1718629},
  url = {https://www.tandfonline.com/doi/full/10.1080/19312458.2020.1718629},
  abstract = {Cronbach's alpha ({$\alpha$}) is a widely-used measure of reliability used to quantify the amount of random measurement error that exists in a sum score or average generated by a multi-item measurement scale. Yet methodologists have warned that {$\alpha$} is not an optimal measure of reliability relative to its more general form, McDonald's omega ({$\omega$}). Among other reasons, that the computation of {$\omega$} is not available as an option in many popular statistics programs and requires items loadings from a confirmatory factor analysis (CFA) have probably hindered more widespread adoption. After a bit of discussion of {$\alpha$} versus {$\omega$}, we illustrate the computation of {$\omega$} using two structural equation modeling programs (Mplus and AMOS) and the MBESS package for R. We then describe a macro for SPSS and SAS (OMEGA) that calculates {$\omega$} in two ways without relying on the estimation of loadings or error variances using CFA. We show that it produces estimates of {$\omega$} that are nearly identical to when using CFA-based estimates of item loadings and error variances. We also discuss the use of the OMEGA macro for certain forms of item analysis and brief form construction based on the removal of items from a longer scale.},
  keywords = {,AMOS,artigoCFA,Omega,Para LER,R software,Reliability,SAS,SPSS},
  file = {C:\Users\pablo\OneDrive\Zotero\Hayes_Coutts_2020_Use Omega Rather than Cronbach‚Äôs Alpha for Estimating Reliability.pdf}
}

@book{henseler2021,
  title = {Composite-{{Based Structural Equation Modeling}}: {{Analyzing Latent}} and {{Emergent Variables}}},
  author = {Henseler, J{\"o}rg},
  year = {2021},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA}
}

@article{holgado-tello2016,
  title = {Robust {{Estimation Methods}} in {{Confirmatory Factor}}  {{Analysis}} of {{Likert Scales}}: {{A Simulation Study}}},
  author = {{Holgado-Tello}, F. and {Morata-Ramirez}, M. and Garc{\'i}a, M.},
  year = {2016},
  journal = {International Review of Social Sciences and Humanities},
  volume = {11},
  number = {2},
  pages = {80--96},
  keywords = {,artigoCFA,CFA,Estimator,Evidencia,ML,Para LER,RML,Robust estimator,RULS,ULS},
  file = {C:\Users\pablo\OneDrive\Zotero\Holgado-Tello et al_2016_Robust Estimation Methods in Confirmatory Factor Analysis of Likert Scales.pdf}
}

@book{hoyle2023,
  title = {Handbook of {{Structural Equation Modeling}}},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral,Importante,Lido,OneNote,Power analysis,SEM},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@incollection{hoyle2023cap1,
  title = {Structural {{Equation Modeling}}: {{An Overview}}},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {Hoyle, Rick H.},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {Guilford Press},
  address = {New Yoirk},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@article{huang2017,
  title = {Asymptotics of {{AIC}}, {{BIC}}, and {{RMSEA}} for {{Model Selection}} in {{Structural Equation Modeling}}},
  author = {Huang, Po-Hsien},
  year = {2017},
  month = jun,
  journal = {Psychometrika},
  volume = {82},
  number = {2},
  pages = {407--426},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-017-9572-y},
  url = {http://link.springer.com/10.1007/s11336-017-9572-y},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {artigoCFA,Importante,Indices,Model comparisons,Para LER},
  file = {C:\Users\pablo\OneDrive\Zotero\Huang_2017_Asymptotics of AIC, BIC, and RMSEA for Model Selection in Structural Equation.pdf}
}

@incollection{hughes2018,
  title = {Psychometric {{Validity}}: {{Establishing}} the {{Accuracy}} and {{Appropriateness}} of {{Psychometric Measures}}},
  booktitle = {The {{Wiley Handbook}} of {{Psychometric Testing}}: {{A Multidisciplinary Reference}} on {{Survey}}, {{Scale}} and {{Test Development}}},
  author = {Hughes, David J.},
  editor = {Irwing, Paul and Booth, Tom and Hughes, David J.},
  year = {2018},
  publisher = {John Wiley \& Sons Ltd.},
  keywords = {,artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Hughes_2018_Psychometric Validity.pdf}
}

@article{jackson2009,
  title = {Reporting Practices in Confirmatory Factor Analysis: {{An}} Overview and Some Recommendations.},
  author = {Jackson, Dennis L. and Gillaspy, J. Arthur and {Purc-Stephenson}, Rebecca},
  year = {2009},
  month = mar,
  journal = {Psychological Methods},
  volume = {14},
  number = {1},
  issn = {1939-1463},
  doi = {10.1037/a0014694},
  keywords = {artigoCFA,CFA,Geral,Importante,Lido},
  file = {C:\Users\pablo\OneDrive\Zotero\Jackson et al_2009_Reporting practices in confirmatory factor analysis.pdf}
}

@article{jak2021,
  title = {Analytical Power Calculations for Structural Equation Modeling: {{A}} Tutorial and {{Shiny}} App},
  author = {Jak, Suzanne and Jorgensen, Terrence D and Verdam, Mathilde G E and Oort, Frans J and Elffers, Louise},
  year = {2021},
  journal = {Behavior Research Mehods},
  volume = {53},
  pages = {1385--1406},
  doi = {10.3758/s13428-020-01479-0/Published},
  url = {https://sjak.shinyapps.io/power4SEM/},
  abstract = {Conducting a power analysis can be challenging for researchers who plan to analyze their data using structural equation models (SEMs), particularly when Monte Carlo methods are used to obtain power. In this tutorial, we explain how power calculations without Monte Carlo methods for the {$\chi$} 2 test and the RMSEA tests of (not-)close fit can be conducted using the Shiny app "power4SEM". power4SEM facilitates power calculations for SEM using two methods that are not computationally intensive and that focus on model fit instead of the statistical significance of (functions of) parameters. These are the method proposed by Satorra and Saris (Psychometrika 50(1), 83-90, 1985) for power calculations of the likelihood ratio test, and that described by MacCallum, Browne, and Sugawara (Psychol Methods 1(2) 130-149, 1996) for RMSEA-based power calculations. We illustrate the use of power4SEM with examples of power analyses for path models, factor models, and a latent growth model.},
  keywords = {,artigoCFA,Importante,Lido,Likelihood ratio test,OneNote,Power analysis,R packages,R software,Root mean square error of approximation,Sample size planning,Structural equation modeling},
  file = {C:\Users\pablo\OneDrive\Zotero\Jak et al_2021_Analytical power calculations for structural equation modeling.pdf}
}

@misc{jamovi2023,
  title = {Jamovi},
  author = {{The jamovi project}},
  year = {2023},
  url = {https://www.jamovi.org},
  urldate = {2021-10-08},
  howpublished = {[Computer Software]},
  keywords = {artigoCFA}
}

@misc{jasp2023,
  title = {{{JASP}}},
  author = {{JASP Team}},
  year = {2023},
  url = {https://jasp-stats.org/},
  urldate = {2023-02-22},
  howpublished = {[Computer Software]},
  keywords = {artigoCFA}
}

@article{jia2019,
  title = {Evaluating Methods for Handling Missing Ordinal Data in Structural Equation Modeling},
  author = {Jia, Fan and Wu, Wei},
  year = {2019},
  month = oct,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {5},
  pages = {2337--2355},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1187-4},
  url = {http://link.springer.com/10.3758/s13428-018-1187-4},
  urldate = {2024-02-01},
  langid = {english},
  keywords = {artigoCFA,Lido,Missing Data,OneNote,SEM},
  file = {C\:\\Users\\pablo\\OneDrive\\Zotero\\Jia_Wu_2019_Evaluating methods for handling missing ordinal data in structural equation.pdf;C\:\\Users\\pablo\\Zotero\\storage\\PXJRAC8N\\Jia and Wu - 2019 - Evaluating methods for handling missing ordinal da.pdf}
}

@article{jobst2023,
  title = {A Tutorial on Assessing Statistical Power and Determining Sample Size for Structural Equation Models.},
  author = {Jobst, Lisa J. and Bader, Martina and Moshagen, Morten},
  year = {2023},
  month = feb,
  journal = {Psychological Methods},
  volume = {28},
  number = {1},
  pages = {207--221},
  issn = {1939-1463},
  doi = {10.1037/met0000423},
  keywords = {artigoCFA,Lido,OneNote,Power analysis,R packages,SEM},
  file = {C:\Users\pablo\OneDrive\Zotero\Jobst et al_2023_A tutorial on assessing statistical power and determining sample size for.pdf}
}

@misc{joreskog2022,
  title = {{{LISREL}} 12 for {{Windows}}},
  author = {J{\"o}reskog, K. G. and S{\"o}rbom, D.},
  year = {2022},
  url = {https://ssicentral.com/index.php/products/lisrel/},
  urldate = {2023-02-22},
  howpublished = {Scientific Software International, Inc},
  keywords = {artigoCFA}
}

@article{kalkbrenner2023,
  title = {Alpha, {{Omega}}, and {{H Internal Consistency Reliability Estimates}}: {{Reviewing These Options}} and {{When}} to {{Use Them}}},
  author = {Kalkbrenner, Michael T.},
  year = {2023},
  month = jan,
  journal = {Counseling Outcome Research and Evaluation},
  volume = {14},
  number = {1},
  pages = {77--88},
  publisher = {Routledge},
  issn = {2150-1378},
  doi = {10.1080/21501378.2021.1940118},
  url = {https://www.tandfonline.com/doi/full/10.1080/21501378.2021.1940118},
  abstract = {Reliability evidence of test scores is essential in counseling research and program evaluation, as the quality of client care is, in part, based on the proper interpretation of test scores. Cronbach's coefficient alpha is unquestionably the most frequently reported estimate of internal consistency reliability in counseling research. For over a decade scholars in other disciplines have raised a number of concerns about the utility of coefficient alpha for capturing the reliability of psychological traits, in favor of composite reliability estimates. However, coefficient alpha remains the most dominant reliability index in counseling research. To this end, this article provides a non-technical summary of coefficient alpha, coefficient omega, hierarchical omega, and coefficient H, guidelines for their appropriate usage, and can serve as a reference for counseling practitioners and researchers when conducting outcome research and program evaluation.},
  keywords = {,artigoCFA,coefficient alpha,composite reliability,counseling,Geral,Importante,Internal consistency reliability,Lido,McDonald's omega,Reliability},
  file = {C:\Users\pablo\OneDrive\Zotero\Kalkbrenner_2023_Alpha, Omega, and H Internal Consistency Reliability Estimates.pdf}
}

@article{kathawalla2021,
  title = {Easing {{Into Open Science}}: {{A Guide}} for {{Graduate Students}} and {{Their Advisors}}},
  shorttitle = {Easing {{Into Open Science}}},
  author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
  year = {2021},
  month = jan,
  journal = {Collabra: Psychology},
  volume = {7},
  number = {1},
  pages = {18684},
  issn = {2474-7394},
  doi = {10.1525/collabra.18684},
  url = {https://online.ucpress.edu/collabra/article/doi/10.1525/collabra.18684/115927/Easing-Into-Open-Science-A-Guide-for-Graduate},
  urldate = {2024-01-25},
  abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
  langid = {english},
  keywords = {artigoCFA,Figuras,Importante,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\Kathawalla et al_2021_Easing Into Open Science.pdf}
}

@article{klein2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jan,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  pages = {20},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  url = {https://online.ucpress.edu/collabra/article/4/1/20/112998/A-Practical-Guide-for-Transparency-in},
  urldate = {2024-01-18},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal -- each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  langid = {english},
  keywords = {artigoCFA,Figuras,Importante,Lido,OneNote,Pratica},
  file = {C:\Users\pablo\OneDrive\Zotero\Klein et al_2018_A Practical Guide for Transparency in Psychological Science.pdf}
}

@book{kline2016,
  title = {Principles and {{Pratice}} of {{Structural Equation Modeling}}},
  author = {Kline, Rex B.},
  year = {2016},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral,Lido,SEM,structural equation modeling},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Kline (2016) - Principles and Practice of Structural Equation Modeling.pdf}
}

@book{kline2023,
  title = {Principles and {{Pratice}} of {{Structural Equation Modeling}}},
  author = {Kline, Rex B.},
  year = {2023},
  edition = {Fifth Edition},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral,Lido,SEM},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Kline (2023) - Principles and Practice of Structural Equation Modeling.pdf}
}

@article{kyriazos2018,
  title = {Applied {{Psychometrics}}: {{Sample Size}} and {{Sample Power Considerations}} in {{Factor Analysis}} ({{EFA}}, {{CFA}}) and {{SEM}} in {{General}}},
  author = {Kyriazos, Theodoros A.},
  year = {2018},
  journal = {Psychology},
  volume = {09},
  number = {08},
  pages = {2207--2230},
  issn = {2152-7180},
  doi = {10.4236/psych.2018.98126},
  keywords = {artigoCFA,Geral,Importante,Lido,OneNote,Power analysis,SEM},
  file = {C:\Users\pablo\OneDrive\Zotero\Kyriazos_2018_Applied Psychometrics.pdf}
}

@article{lai2020categorical,
  title = {Correct {{Point Estimator}} and {{Confidence Interval}} for {{RMSEA Given Categorical Data}}},
  author = {Lai, Keke},
  year = {2020},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {5},
  pages = {678--695},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2019.1687302},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2019.1687302},
  urldate = {2024-02-06},
  langid = {english},
  keywords = {artigoCFA,Estimator,Indices,Ordinal data,RMSEA,ULS}
}

@article{lai2020nonnested,
  title = {Confidence {{Interval}} for {{RMSEA}} or {{CFI Difference Between Nonnested Models}}},
  author = {Lai, Keke},
  year = {2020},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {1},
  pages = {16--32},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2019.1631704},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2019.1631704},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {artigoCFA,Indices,Model comparisons},
  file = {C:\Users\pablo\OneDrive\Zotero\Lai_2020_Confidence Interval for RMSEA or CFI Difference Between Nonnested Models.pdf}
}

@article{lai2021fit,
  title = {Fit {{Difference Between Nonnested Models Given Categorical Data}}: {{Measures}} and {{Estimation}}},
  shorttitle = {Fit {{Difference Between Nonnested Models Given Categorical Data}}},
  author = {Lai, Keke},
  year = {2021},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {28},
  number = {1},
  pages = {99--120},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2020.1763802},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2020.1763802},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {artigoCFA,Indices,Model comparisons,Ordinal data},
  file = {C:\Users\pablo\OneDrive\Zotero\Lai_2021_Fit Difference Between Nonnested Models Given Categorical Data.pdf}
}

@article{lai2021missing,
  title = {Using {{Information Criteria Under Missing Data}}: {{Full Information Maximum Likelihood Versus Two-Stage Estimation}}},
  shorttitle = {Using {{Information Criteria Under Missing Data}}},
  author = {Lai, Keke},
  year = {2021},
  month = mar,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {28},
  number = {2},
  pages = {278--291},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2020.1780925},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2020.1780925},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {artigoCFA,Indices,Missing Data,Model comparisons},
  file = {C:\Users\pablo\OneDrive\Zotero\Lai_2021_Using Information Criteria Under Missing Data.pdf}
}

@article{lei2020,
  title = {Performance of {{Estimators}} for {{Confirmatory Factor Analysis}} of {{Ordinal Variables}} with {{Missing Data}}},
  author = {Lei, Pui-Wa and Shiverdecker, Levi K.},
  year = {2020},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {4},
  pages = {584--601},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2019.1680292},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2019.1680292},
  abstract = {Missing data and ordinal indicators are common in applied research involving latent constructs. Unfortunately, ordinal indicators violate the linearity assumption for conventional CFA that is routinely used to provide structural validity evidence for measurement instruments. Although robust maximum likelihood estimator (MLR) can deal with both missing data and nonnormality, it is generally inappropriate for ordinal indicators. Categorical estimation methods such as weighted least square mean and variance adjusted (WLSMV) method, or MLR or maximum likelihood (ML) that justly treats ordinal indicators as categorical (MLR-CAT or ML-CAT, respectively) have been recommended for ordinal dependent variables. However, performances of these categorical estimators in the presence of missing data have not been empirically examined. The current study systematically investigates the relative performances of WLSMV, MLR, MLR-CAT, and ML-CAT under different conditions of missing data amount and mechanism, sample size, level of indicator distribution, and number of indicator categories. Results generally favor MLR-CAT so long as the sample size is not too small ({$>$}200) to result in convergence problems.},
  keywords = {,artigoCFA,Categorical estimator,Importante,Lido,Missing Data,OneNote,ordinal data,robust estimator},
  file = {C:\Users\pablo\OneDrive\Zotero\Lei_Shiverdecker_2020_Performance of Estimators for Confirmatory Factor Analysis of Ordinal Variables.pdf}
}

@incollection{leite2023,
  title = {Simulation {{Methods}} in {{Structural Equation Modeling}}},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {Leite, Walter L. and Bandalos, Deborah L. and Shen, Zuchao},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Geral,Importante,Lido,OneNote,SEM,Simulation},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@article{li2016cfa,
  title = {Confirmatory Factor Analysis with Ordinal Data: {{Comparing}} Robust Maximum Likelihood and Diagonally Weighted Least Squares},
  author = {Li, Cheng Hsien},
  year = {2016},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {3},
  pages = {936--949},
  publisher = {Springer New York LLC},
  issn = {15543528},
  doi = {10.3758/s13428-015-0619-7},
  url = {https://link.springer.com/article/10.3758/s13428-015-0619-7},
  urldate = {2021-03-22},
  abstract = {In confirmatory factor analysis (CFA), the use of maximum likelihood (ML) assumes that the observed indicators follow a continuous and multivariate normal distribution, which is not appropriate for ordinal observed variables. Robust ML (MLR) has been introduced into CFA models when this normality assumption is slightly or moderately violated. Diagonally weighted least squares (WLSMV), on the other hand, is specifically designed for ordinal data. Although WLSMV makes no distributional assumptions about the observed variables, a normal latent distribution underlying each observed categorical variable is instead assumed. A Monte Carlo simulation was carried out to compare the effects of different configurations of latent response distributions, numbers of categories, and sample sizes on model parameter estimates, standard errors, and chi-square test statistics in a correlated two-factor model. The results showed that WLSMV was less biased and more accurate than MLR in estimating the factor loadings across nearly every condition. However, WLSMV yielded moderate overestimation of the interfactor correlations when the sample size was small or/and when the latent distributions were moderately nonnormal. With respect to standard error estimates of the factor loadings and the interfactor correlations, MLR outperformed WLSMV when the latent distributions were nonnormal with a small sample size of N = 200. Finally, the proposed model tended to be over-rejected by chi-square test statistics under both MLR and WLSMV in the condition of small sample size N = 200.},
  pmid = {26174714},
  keywords = {,artigoCFA,CFA,Confirmatory factor analysis,Estimator,Evidencia,Importante,ML,MLR,Monte Carlo Simulation,Ordinal data,Para LER,Robust estimation,WLSMV},
  file = {C:\Users\pablo\OneDrive\Zotero\Li_2016_Confirmatory factor analysis with ordinal data.pdf}
}

@article{li2016sem,
  title = {The Performance of {{ML}}, {{DWLS}}, and {{ULS}} Estimation with Robust Corrections in Structural Equation Models with Ordinal Variables},
  author = {Li, Cheng Hsien},
  year = {2016},
  month = sep,
  journal = {Psychological Methods},
  volume = {21},
  number = {3},
  pages = {369--387},
  publisher = {American Psychological Association Inc.},
  issn = {1082989X},
  doi = {10.1037/met0000093},
  urldate = {2021-03-23},
  abstract = {Three estimation methods with robust corrections-maximum likelihood (ML) using the sample covariance matrix, unweighted least squares (ULS) using a polychoric correlation matrix, and diagonally weighted least squares (DWLS) using a polychoric correlation matrix-have been proposed in the literature, and are considered to be superior to normal theory-based maximum likelihood when observed variables in latent variable models are ordinal. A Monte Carlo simulation study was carried out to compare the performance of ML, DWLS, and ULS in estimating model parameters, and their robust corrections to standard errors, and chi-square statistics in a structural equation model with ordinal observed variables. Eighty-four conditions, characterized by different ordinal observed distribution shapes, numbers of response categories, and sample sizes were investigated. Results reveal that (a) DWLS and ULS yield more accurate factor loading estimates than ML across all conditions; (b) DWLS and ULS produce more accurate interfactor correlation estimates than ML in almost every condition; (c) structural coefficient estimates from DWLS and ULS outperform ML estimates in nearly all asymmetric data conditions; (d) robust standard errors of parameter estimates obtained with robust ML are more accurate than those produced by DWLS and ULS across most conditions; and (e) regarding robust chi-square statistics, robust ML is inferior to DWLS and ULS in controlling for Type I error in almost every condition, unless a large sample is used (N = 1,000). Finally, implications of the findings are discussed, as are the limitations of this study as well as potential directions for future research.},
  pmid = {27571021},
  keywords = {,artigoCFA,Diagonally weighted least squares,DWLS,Estimator,Evidencia,Importante,Maximum likelihood,ML,Ordinal data,Para LER,RDWLS,RML,Robust estimator,Robust statistics,RULS,SEM,ULS,Unweighted least squares},
  file = {C:\Users\pablo\OneDrive\Zotero\Li_2016_The performance of ML, DWLS, and ULS estimation with robust corrections in.pdf}
}

@article{lim2022,
  title = {Evaluating {{FIML}} and Multiple Imputation in Joint Ordinal-Continuous Measurements Models with Missing Data},
  author = {Lim, Aaron J.-M. and Cheung, Mike W.-L.},
  year = {2022},
  journal = {Behavior Research Methods},
  volume = {54},
  pages = {1063--1077},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01582-w},
  url = {https://link.springer.com/10.3758/s13428-021-01582-w},
  urldate = {2024-01-30},
  langid = {english},
  keywords = {artigoCFA,CFA,Importante,Missing Data,Ordinal data,Para LER},
  file = {C:\Users\pablo\OneDrive\Zotero\Lim_Cheung_2021_Evaluating FIML and multiple imputation in joint ordinal-continuous.pdf}
}

@article{mai2021,
  title = {A Tailored-Fit Model Evaluation Strategy for Better Decisions about Structural Equation Models},
  author = {Mai, Robert and Niemand, Thomas and Kraus, Sascha},
  year = {2021},
  month = dec,
  journal = {Technological Forecasting and Social Change},
  volume = {173},
  pages = {121142},
  issn = {00401625},
  doi = {10.1016/j.techfore.2021.121142},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0040162521005758},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {artigoCFA,Importante,Indices,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\Mai et al_2021_A tailored-fit model evaluation strategy for better decisions about structural.pdf}
}

@incollection{mang2023,
  title = {Reproducibility in 2023 -- {{An End-to-End Template}} for {{Analysis}} and {{Manuscript Writing}}},
  booktitle = {Studies in {{Health Technology}} and {{Informatics}}},
  author = {Mang, Jonathan M. and Prokosch, Hans-Ulrich and Kapsner, Lorenz A.},
  editor = {H{\"a}gglund, Maria and Blusi, Madeleine and Bonacina, Stefano and Nilsson, Lina and Cort Madsen, Inge and Pelayo, Sylvia and Moen, Anne and Benis, Arriel and Lindsk{\"o}ld, Lars and Gallos, Parisis},
  year = {2023},
  month = may,
  publisher = {IOS Press},
  doi = {10.3233/SHTI230064},
  url = {https://ebooks.iospress.nl/doi/10.3233/SHTI230064},
  urldate = {2024-01-26},
  abstract = {Reproducibility imposes some special requirements at different stages of each project, including reproducible workflows for the analysis including to follow best practices regarding code style and to make the creation of the manuscript reproducible as well. Available tools therefore include version control systems such as Git and document creation tools such as Quarto or R Markdown. However, a re-usable project template mapping the entire process from performing the data analysis to finally writing the manuscript in a reproducible manner is yet lacking. This work aims to fill this gap by presenting an open source template for conducting reproducible research projects utilizing a containerized framework for both developing and conducting the analysis and summarizing the results in a manuscript. This template can be used instantly without any customization.},
  isbn = {978-1-64368-388-1 978-1-64368-389-8},
  keywords = {artigoCFA,Figuras,LIdo,OneNote,Quarto,R workflow},
  file = {C:\Users\pablo\OneDrive\Zotero\Mang et al_2023_Reproducibility in 2023 ‚Äì An End-to-End Template for Analysis and Manuscript.pdf}
}

@article{marcoulides2017,
  title = {New {{Ways}} to {{Evaluate Goodness}} of {{Fit}}: {{A Note}} on {{Using Equivalence Testing}} to {{Assess Structural Equation Models}}},
  author = {Marcoulides, Katerina M. and Yuan, Ke Hai},
  year = {2017},
  month = jan,
  journal = {Structural Equation Modeling},
  volume = {24},
  number = {1},
  pages = {148--153},
  publisher = {Routledge},
  issn = {15328007},
  doi = {10.1080/10705511.2016.1225260},
  abstract = {Structural equation models are typically evaluated on the basis of goodness-of-fit indexes. Despite their popularity, agreeing what value these indexes should attain to confidently decide between the acceptance and rejection of a model has been greatly debated. A recently proposed approach by means of equivalence testing has been recommended as a superior way to evaluate the goodness of fit of models. The approach has also been proposed as providing a necessary vehicle that can be used to advance the inferential nature of structural equation modeling as a confirmatory tool. The purpose of this article is to introduce readers to key ideas in equivalence testing and illustrate its use for conducting model--data fit assessments. Two confirmatory factor analysis models in which a priori specified latent variable models with known structure and tested against data are used as examples. It is advocated that whenever the goodness of fit of a model is to be assessed researchers should always examine the resulting values obtained via the equivalence testing approach.},
  keywords = {,artigoCFA,CFI,EQT,equivalence testing,fit indexes,Importante,Indices,Lido,likelihood ratio statistic,OneNote,RMSEA},
  file = {C:\Users\pablo\OneDrive\Zotero\Marcoulides_Yuan_2017_New Ways to Evaluate Goodness of Fit.pdf}
}

@article{marsh2004,
  title = {Why {{Multicollinearity Matters}}: {{A Reexamination}} of {{Relations Between Self-Efficacy}}, {{Self-Concept}}, and {{Achievement}}.},
  shorttitle = {Why {{Multicollinearity Matters}}},
  author = {Marsh, Herbert W. and Dowson, Martin and Pietsch, James and Walker, Richard},
  year = {2004},
  month = sep,
  journal = {Journal of Educational Psychology},
  volume = {96},
  number = {3},
  pages = {518--522},
  issn = {1939-2176, 0022-0663},
  doi = {10.1037/0022-0663.96.3.518},
  url = {https://doi.apa.org/doi/10.1037/0022-0663.96.3.518},
  urldate = {2024-02-07},
  langid = {english},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Marsh et al_2004_Why Multicollinearity Matters.pdf}
}

@article{martins2021,
  title = {Tutorial-{{Articles}}: {{The Importance}} of {{Data}} and {{Code Sharing}}},
  shorttitle = {Tutorial-{{Articles}}},
  author = {Martins, Henrique Castro},
  year = {2021},
  journal = {Revista de Administra{\c c}{\~a}o Contempor{\^a}nea},
  volume = {25},
  number = {1},
  pages = {e200212},
  issn = {1982-7849, 1415-6555},
  doi = {10.1590/1982-7849rac2021200212},
  url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S1415-65552021000100801&tlng=en},
  urldate = {2024-01-26},
  abstract = {ABSTRACT  Context: this document is designed to be along with those that are in the first edition of the new section of the Journal of Contemporary Administration (RAC): the tutorial-articles section.  Objective: the purpose is to present the new section and discuss relevant topics of tutorial-articles.  Method: I divide the document into three main parts. First, I provide a summary of the state of the art in open data and open code at the current date that, jointly, create the context for tutorial-articles. Second, I provide some guidance to the future of the section on tutorial-articles, providing a structure and some insights that can be developed in the future. Third, I offer a short R script to show examples of open data that, I believe, can be used in the future in tutorial-articles, but also in innovative empirical studies.  Conclusion: finally, I provide a short description of the first tutorial-articles accepted for publication in this current RAC's edition.           ,              RESUMO  Contexto: este documento foi escrito para compor a primeira edi{\c c}{\~a}o da nova se{\c c}{\~a}o da Revista de Administra{\c c}{\~a}o Contempor{\^a}nea (RAC): a se{\c c}{\~a}o de artigos-tutoriais.  Objetivo: o objetivo deste artigo {\'e} apresentar a nova se{\c c}{\~a}o e discutir t{\'o}picos relevantes a um artigo-tutorial.  M{\'e}todo: este documento {\'e} dividido em tr{\^e}s partes principais. Primeiro, oferece-se um resumo das mais importantes pr{\'a}ticas de dados abertos e materiais abertos atualmente que, conjuntamente, criam o contexto ideal para artigos-tutoriais. Em seguida, oferecem-se diretrizes para o futuro da se{\c c}{\~a}o e algumas ideias que podem ser desenvolvidas no futuro. Em seguida, oferece-se um protocolo de pesquisa em R com exemplos de bases de dados abertas, que, acredita-se, podem ser relevantes para artigos-tutoriais futuros, mas tamb{\'e}m para estudos emp{\'i}ricos diversos.  Conclus{\~a}o: finalmente, ofere{\c c}o uma breve descri{\c c}{\~a}o dos artigos-tutoriais aceitos na presente se{\c c}{\~a}o da RAC.},
  keywords = {Artigo-Tutorial,artigoCFA,Importante,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\Martins_2021_Tutorial-Articles.pdf}
}

@article{maydeu-olivares2017a,
  title = {Goodness of {{Fit}} in {{Item Factor Analysis}}: {{Effect}} of the {{Number}} of {{Response Alternatives}}},
  author = {{Maydeu-Olivares}, Alberto and Fairchild, Amanda J. and Hall, Alexander G.},
  year = {2017},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {24},
  number = {4},
  pages = {495--505},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2017.1289816},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2017.1289816},
  urldate = {2021-04-13},
  abstract = {The power of the chi-square test statistic used in structural equation modeling decreases as the absolute value of excess kurtosis of the observed data increases. Excess kurtosis is more likely the smaller the number of item response categories. As a result, fit is likely to improve as the number of item response categories decreases, regardless of the true underlying factor structure or {$\chi$}2-based fit index used to examine model fit. Equivalently, given a target value of approximate fit (e.g., root mean square error of approximation {$\leq$}.05) a model with more factors is needed to reach it as the number of categories increases. This is true regardless of whether the data are treated as continuous (common factor analysis) or as discrete (ordinal factor analysis). We recommend using a large number of response alternatives ({$\geq$} 5) to increase the power to detect incorrect substantive models.},
  keywords = {,artigoCFA,difficulty factors,Evidencia,Importante,Likert,model fit,Ordinal data,Para LER,SEM},
  file = {C:\Users\pablo\OneDrive\Zotero\Maydeu-Olivares et al_2017_Goodness of Fit in Item Factor Analysis.pdf}
}

@article{mcneish2018,
  title = {Thanks Coefficient Alpha, {{We}}'ll Take It from Here},
  author = {McNeish, Daniel},
  year = {2018},
  month = sep,
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {412--433},
  publisher = {American Psychological Association Inc.},
  issn = {1082989X},
  doi = {10.1037/met0000144},
  abstract = {Empirical studies in psychology commonly report Cronbach's alpha as a measure of internal consistency reliability despite the fact that many methodological studies have shown that Cronbach's alpha is riddled with problems stemming from unrealistic assumptions. In many circumstances, violating these assumptions yields estimates of reliability that are too small, making measures look less reliable than they actually are. Although methodological critiques of Cronbach's alpha are being cited with increasing frequency in empirical studies, in this tutorial we discuss how the trend is not necessarily improving methodology used in the literature. That is, many studies continue to use Cronbach's alpha without regard for its assumptions or merely cite methodological articles advising against its use to rationalize unfavorable Cronbach's alpha estimates. This tutorial first provides evidence that recommendations against Cronbach's alpha have not appreciably changed how empirical studies report reliability. Then, we summarize the drawbacks of Cronbach's alpha conceptually without relying on mathematical or simulation-based arguments so that these arguments are accessible to a broad audience. We continue by discussing several alternative measures that make less rigid assumptions which provide justifiably higher estimates of reliability compared to Cronbach's alpha. We conclude with empirical examples to illustrate advantages of alternative measures of reliability including omega total, Revelle's omega total, the greatest lower bound, and Coefficient H. A detailed software appendix is also provided to help researchers implement alternative methods.},
  pmid = {28557467},
  keywords = {Alpha,artigoCFA,Cronbach's alpha,Geral,GLB,Importante,Internal consistency,Omega,Para LER,Psychometrics,R software,Reliability},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_2018_Thanks coefficient alpha, We'll take it from here.pdf}
}

@article{mcneish20232nd,
  title = {Dynamic Fit Index Cutoffs for Hierarchical and Second-Order Factor Models},
  author = {McNeish, Daniel and Manapat, Patrick D.},
  year = {2023},
  journal = {PsyArXiv Preprints},
  url = {https://doi.org/10.31234/osf.io/sm6az},
  urldate = {2023-11-01},
  keywords = {artigoCFA,Bifactor,CFA,DFI,Importante,Indices,Para LER,second-order factor analysis},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_Manapat_2023_Dynamic fit index cutoffs for hierarchical and second-order factor models.pdf}
}

@article{mcneish2023geral,
  title = {Generalizability of {{Dynamic Fit Index}}, {{Equivalence Testing}}, and {{Hu}} \& {{Bentler Cutoffs}} for {{Evaluating Fit}} in {{Factor Analysis}}},
  author = {McNeish, Daniel},
  year = {2023},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {58},
  number = {1},
  pages = {195--219},
  issn = {0027-3171},
  doi = {10.1080/00273171.2022.2163477},
  keywords = {artigoCFA,CFA,DFI,EQT,Importante,Indices,Lido,OneNote,R packages},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_2023_Generalizability of Dynamic Fit Index, Equivalence Testing, and Hu & Bentler.pdf}
}

@article{mcneish2023likert,
  title = {Dynamic {{Fit Index Cutoffs}} for {{Factor Analysis}} with {{Likert}}, {{Ordinal}}, or {{Binary Responses}}},
  author = {McNeish, Daniel},
  year = {2023},
  journal = {PsyArXiv Preprints},
  publisher = {Arizona State University},
  address = {Tempe},
  url = {https://doi.org/10.31234/osf.io/tp35s},
  urldate = {2023-11-01},
  keywords = {artigoCFA,DFI,Importante,Indices,Lido,OneNote,R packages},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_2023_Dynamic Fit Index Cutoffs for Factor Analysis with Likert, Ordinal, or Binary.pdf}
}

@article{mcneishwolf2022,
  title = {Dynamic Fit Index Cutoffs for One-Factor Models},
  author = {McNeish, Daniel and Wolf, Melissa G.},
  year = {2022},
  month = may,
  journal = {Behavior Research Methods},
  volume = {55},
  number = {3},
  pages = {1157--1174},
  issn = {1554-3528},
  doi = {10.3758/s13428-022-01847-y},
  keywords = {artigoCFA,CFA,DFI,Importante,Indices,Para LER},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_Wolf_2022_Dynamic fit index cutoffs for one-factor models.pdf}
}

@article{mcneishwolf2023cfa,
  title = {Dynamic Fit Index Cutoffs for Confirmatory Factor Analysis Models.},
  author = {McNeish, Daniel and Wolf, Melissa G.},
  year = {2023},
  month = feb,
  journal = {Psychological Methods},
  volume = {28},
  number = {1},
  pages = {61--88},
  issn = {1939-1463},
  doi = {10.1037/met0000425},
  keywords = {,artigoCFA,CFA,DFI,Importante,Indices,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_Wolf_2023_Dynamic fit index cutoffs for confirmatory factor analysis models.pdf}
}

@techreport{mcneishwolf2023dddf,
  type = {Preprint},
  title = {Direct {{Discrepancy Dynamic Fit Index Cutoffs}} for {{Arbitrary Covariance Structure Models}}},
  author = {McNeish, Daniel and Wolf, Melissa Gordon},
  year = {2023},
  month = sep,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/4r9fq},
  url = {https://osf.io/4r9fq},
  urldate = {2024-02-23},
  abstract = {Despite the popularity of traditional fit index cutoffs like RMSEA {$\leq$} .06 and CFI {$\geq$} .95, several studies have noted issues with overgeneralizing traditional cutoffs. Computational methods have been proposed to avoid overgeneralization by deriving cutoffs specifically tailored to characteristics of the model being evaluated. Simulations show favorable performance of these methods; however, these methods support a narrow set of models and response scales (i.e., only continuous variables) and interpretation of cutoffs is not always standardized, which affects empirical researchers' ability to confidently and broadly adopt these methods to evaluate model fit. In this paper, we propose an extension to one recently developed computational method---dynamic fit index cutoffs---that (a) permits application to any covariance structure model (e.g., CFA, mediation, bifactor), (b) standardizes interpretation of cutoffs across any covariance structure model, and (c) supports normal, nonnormal, categorical, and missing data. Software is provided to facilitate implementation of the method.},
  keywords = {artigoCFA,Importante,Indices,Para LER},
  file = {C:\Users\pablo\OneDrive\Zotero\McNeish_Wolf_2023_Direct Discrepancy Dynamic Fit Index Cutoffs for Arbitrary Covariance Structure.pdf}
}

@article{mendes-da-silva2023,
  title = {What {{Lectures}} and {{Research}} in {{Business Management Need}} to {{Know About Open Science}}},
  author = {{Mendes-Da-Silva}, Wesley},
  year = {2023},
  journal = {Revista de Administra{\c c}{\~a}o de Empresas},
  volume = {63},
  number = {4},
  pages = {e0000-0033},
  issn = {2178-938X, 0034-7590},
  doi = {10.1590/s0034-759020230408x},
  url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0034-75902023000400100&tlng=en},
  urldate = {2024-01-24},
  keywords = {Artigo-Tutorial,artigoCFA,Figuras,Importante,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\Mendes-Da-Silva_2023_What Lectures and Research IN Business Management Need to Know About Open.pdf}
}

@article{moshagen2023,
  title = {{{semPower}}: {{General}} Power Analysis for Structural Equation Models},
  shorttitle = {{{semPower}}},
  author = {Moshagen, Morten and Bader, Martina},
  year = {2023},
  month = nov,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02254-7},
  url = {https://link.springer.com/10.3758/s13428-023-02254-7},
  urldate = {2024-02-16},
  abstract = {Abstract             Structural equation modeling (SEM) is a widespread and commonly used approach to test substantive hypotheses in the social and behavioral sciences. When performing hypothesis tests, it is vital to rely on a sufficiently large sample size to achieve an adequate degree of statistical power to detect the hypothesized effect. However, applications of SEM rarely consider statistical power in informing sample size considerations or determine the statistical power for the focal hypothesis tests performed. One reason is the difficulty in translating substantive hypotheses into specific effect size values required to perform power analyses, as well as the lack of user-friendly software to automate this process. The present paper presents the second version of the R package semPower which includes comprehensive functionality for various types of power analyses in SEM. Specifically, semPower 2 allows one to perform both analytical and simulated a priori, post hoc, and compromise power analysis for structural equation models with or without latent variables, and also supports multigroup settings and provides user-friendly convenience functions for many common model types (e.g., standard confirmatory factor analysis [CFA] models, regression models, autoregressive moving average [ARMA] models, cross-lagged panel models) to simplify power analyses when a model-based definition of the effect in terms of model parameters is desired.},
  langid = {english},
  keywords = {artigoCFA,Lido,OneNote,Power analysis,R packages,Simulation},
  file = {C:\Users\pablo\OneDrive\Zotero\Moshagen_Bader_2023_semPower2.pdf}
}

@misc{muthen2023,
  title = {Mplus Version 8.9 User's Guide},
  author = {Muth{\'e}n, L. K. and Muth{\'e}n, B. O.},
  year = {2023},
  keywords = {artigoCFA}
}

@article{nalbantoglu-yilmaz2019,
  title = {Comparison of {{Different Estimation Methods Used}} in {{Confirmatory Factor Analyses}} in {{Non-Normal Data}}: {{A Monte Carlo Study}}},
  author = {{Nalbanto{\u g}lu-Y{\i}lmaz}, Funda},
  year = {2019},
  journal = {International Online Journal of Educational Sciences},
  volume = {11},
  number = {4},
  issn = {13092707},
  doi = {10.15345/iojes.2019.04.010},
  keywords = {,artigoCFA,CFA,Estimator,Evidencia,ML,MLR,Robust estimator,WLS},
  file = {C:\Users\pablo\OneDrive\Zotero\Nalbantoƒülu Yƒ±lmaz_2019_Comparison of Different Estimation Methods Used in Confirmatory Factor Analyses.pdf}
}

@article{neale2016,
  title = {{{OpenMx}} 2.0: {{Extended Structural Equation}} and {{Statistical Modeling}}},
  shorttitle = {{{OpenMx}} 2.0},
  author = {Neale, Michael C. and Hunter, Michael D. and Pritikin, Joshua N. and Zahery, Mahsa and Brick, Timothy R. and Kirkpatrick, Robert M. and Estabrook, Ryne and Bates, Timothy C. and Maes, Hermine H. and Boker, Steven M.},
  year = {2016},
  month = jun,
  journal = {Psychometrika},
  volume = {81},
  number = {2},
  pages = {535--549},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-014-9435-8},
  url = {http://link.springer.com/10.1007/s11336-014-9435-8},
  urldate = {2024-01-16},
  langid = {english},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Neale et al_2016_OpenMx 2.pdf}
}

@article{niemand2018,
  title = {Flexible Cutoff Values for Fit Indices in the Evaluation of Structural Equation Models},
  author = {Niemand, Thomas and Mai, Robert},
  year = {2018},
  month = nov,
  journal = {Journal of the Academy of Marketing Science},
  volume = {46},
  number = {6},
  pages = {1148--1172},
  issn = {0092-0703, 1552-7824},
  doi = {10.1007/s11747-018-0602-9},
  url = {http://link.springer.com/10.1007/s11747-018-0602-9},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {artigoCFA,Importante,Indices,Lido,OneNote},
  file = {C:\Users\pablo\OneDrive\Zotero\Niemand_Mai_2018_Flexible cutoff values for fit indices in the evaluation of structural equation.pdf}
}

@article{nye2022,
  title = {Reviewer {{Resources}}: {{Confirmatory Factor Analysis}}},
  author = {Nye, Christopher D.},
  year = {2022},
  month = aug,
  journal = {Organizational Research Methods},
  pages = {109442812211205},
  issn = {1094-4281},
  doi = {10.1177/10944281221120541},
  url = {http://journals.sagepub.com/doi/10.1177/10944281221120541},
  abstract = {{$<$}p{$>$}Confirmatory factor analyses (CFA) are widely used in the organizational literature. As a result, understanding how to properly conduct these analyses, report the results, and interpret their implications is critically important for advancing organizational research. The goal of this paper is to summarize the complexities of CFA models and, therefore, to provide a resource for journal reviewers and researchers who are using CFA in their research. The topics covered in this paper include the estimation process, power analyses, model fit, and model modifications, among other things. In addition, this paper concludes with a checklist that summarizes the key points that are discussed and can be used to evaluate future studies that incorporate CFA.{$<$}/p{$>$}},
  keywords = {,artigoCFA,CFA,Geral,Importante,Lido},
  file = {C:\Users\pablo\OneDrive\Zotero\Nye_2022_Reviewer Resources.pdf}
}

@article{peikert2021,
  title = {A {{Reproducible Data Analysis Workflow With R Markdown}}, {{Git}}, {{Make}}, and {{Docker}}},
  author = {Peikert, Aaron and Brandmaier, Andreas M.},
  year = {2021},
  month = may,
  journal = {Quantitative and Computational Methods in Behavioral Sciences},
  volume = {1},
  pages = {e3763},
  issn = {2699-8432},
  doi = {10.5964/qcmb.3763},
  url = {https://qcmb.psychopen.eu/index.php/qcmb/article/view/3763},
  urldate = {2024-01-28},
  abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles, and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), 2) the analysis exactly reproduces at a later point in time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible. While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Combining containerization, dependence management, version management, and dynamic document generation, the proposed workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
  keywords = {artigoCFA,Figuras,Importante,Lido,OneNote,R workflow},
  file = {C:\Users\pablo\OneDrive\Zotero\Peikert_Brandmaier_2021_A Reproducible Data Analysis Workflow.pdf}
}

@article{peikert2021a,
  title = {Reproducible {{Research}} in {{R}}: {{A Tutorial}} on {{How}} to {{Do}} the {{Same Thing More Than Once}}},
  shorttitle = {Reproducible {{Research}} in {{R}}},
  author = {Peikert, Aaron and Van Lissa, Caspar J. and Brandmaier, Andreas M.},
  year = {2021},
  month = dec,
  journal = {Psych},
  volume = {3},
  number = {4},
  pages = {836--867},
  issn = {2624-8611},
  doi = {10.3390/psych3040053},
  url = {https://www.mdpi.com/2624-8611/3/4/53},
  urldate = {2024-01-18},
  abstract = {Computational reproducibility is the ability to obtain identical results from the same data with the same computer code. It is a building block for transparent and cumulative science because it enables the originator and other researchers, on other computers and later in time, to reproduce and thus understand how results came about, while avoiding a variety of errors that may lead to erroneous reporting of statistical and computational results. In this tutorial, we demonstrate how the R package repro supports researchers in creating fully computationally reproducible research projects with tools from the software engineering community. Building upon this notion of fully automated reproducibility, we present several applications including the preregistration of research plans with code (Preregistration as Code, PAC). PAC eschews all ambiguity of traditional preregistration and offers several more advantages. Making technical advancements that serve reproducibility more widely accessible for researchers holds the potential to innovate the research process and to help it become more productive, credible, and reliable.},
  langid = {english},
  keywords = {artigoCFA,Importante,Lido,OneNote,R workflow},
  file = {C:\Users\pablo\OneDrive\Zotero\Peikert et al_2021_Reproducible Research in R.pdf}
}

@article{pilcher2023,
  title = {'{{Qualitative}}' and 'quantitative' Methods and Approaches across Subject Fields: Implications for Research Values, Assumptions, and Practices},
  shorttitle = {'{{Qualitative}}' and 'quantitative' Methods and Approaches across Subject Fields},
  author = {Pilcher, Nick and Cortazzi, Martin},
  year = {2023},
  month = sep,
  journal = {Quality \& Quantity},
  issn = {0033-5177, 1573-7845},
  doi = {10.1007/s11135-023-01734-4},
  url = {https://link.springer.com/10.1007/s11135-023-01734-4},
  urldate = {2024-03-02},
  abstract = {Abstract             There is considerable literature showing the complexity, connectivity and blurring of 'qualitative' and 'quantitative' methods in research. Yet these concepts are often represented in a binary way as independent dichotomous categories. This is evident in many key textbooks which are used in research methods courses to guide students and newer researchers in their research training. This paper analyses such textbook representations of 'qualitative' and 'quantitative' in 25 key resources published in English (supported by an outline survey of 23 textbooks written in German, Spanish and French). We then compare these with the perceptions, gathered through semi-structured interviews, of university researchers (n\,=\,31) who work in a wide range of arts and science disciplines. The analysis of what the textbooks say compared to what the participants report they do in their practice shows some common features, as might be assumed, but there are significant contrasts and contradictions. The differences tend to align with some other recent literature to underline the complexity and connectivity associated with the terms. We suggest ways in which future research methods courses and newer researchers could question and positively deconstruct such binary representations in order to free up directions for research in practice, so that investigations can use both quantitative or qualitative approaches in more nuanced practices that are appropriate to the specific field and given context of investigations.},
  langid = {english},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Pilcher_Cortazzi_2023_'Qualitative' and 'quantitative' methods and approaches across subject fields.pdf}
}

@article{podsakoff2012,
  title = {Sources of {{Method Bias}} in {{Social Science Research}} and {{Recommendations}} on {{How}} to {{Control It}}},
  author = {Podsakoff, Philip M. and MacKenzie, Scott B. and Podsakoff, Nathan P.},
  year = {2012},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {63},
  number = {1},
  pages = {539--569},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-120710-100452},
  abstract = {Despite the concern that has been expressed about potential method biases, and the pervasiveness of research settings with the potential to produce them, there is disagreement about whether they really are a problem for researchers in the behavioral sciences. Therefore, the purpose of this review is to explore the current state of knowledge about method biases. First, we explore the meaning of the terms ``method'' and ``method bias'' and then we examine whether method biases influence all measures equally. Next, we review the evidence of the effects that method biases have on individual measures and on the covariation between different constructs. Following this, we evaluate the procedural and statistical remedies that have been used to control method biases and provide recommendations for minimizing method bias.},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Podsakoff et al_2012_Sources of Method Bias in Social Science Research and Recommendations on How to.pdf}
}

@misc{pornprasertmanit2022,
  title = {Simsem: {{SIMulated Structural Equation Modeling}}},
  author = {Pornprasertmanit, Sunthud and Miller, Patrick and Jorgensen, Terrence D. and Corbin, Quick},
  year = {2022},
  url = {www.simsem.org},
  urldate = {2023-10-20},
  howpublished = {R package},
  keywords = {artigoCFA}
}

@incollection{preacher2023,
  title = {Model {{Selection}} in {{Structural Equation Modeling}}},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {Preacher, Kristopher J. and Yaremych, Haley E.},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Importante,Indices,Lido,Model comparisons,OneNote,SEM},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@book{price2017,
  title = {Psychometric {{Methods}}: {{Theory}} into {{Practice}}},
  author = {Price, Larry R.},
  year = {2017},
  series = {Methodology in the Social Sciences},
  edition = {1st Edition},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Books\Psychometrics\Psychometric Methods_ Theory into Practice - Price (2017).pdf}
}

@article{reeves2016,
  title = {Contemporary Test Validity in Theory and Practice: {{A}} Primer for Discipline-Based Education Researchers},
  author = {Reeves, Todd D. and {Marbach-Ad}, Gili},
  year = {2016},
  month = mar,
  journal = {CBE Life Sciences Education},
  volume = {15},
  number = {1},
  publisher = {American Society for Cell Biology},
  issn = {19317913},
  doi = {10.1187/cbe.15-08-0183},
  abstract = {Most discipline-based education researchers (DBERs) were formally trained in the methods of scientific disciplines such as biology, chemistry, and physics, rather than social science disciplines such as psychology and education. As a result, DBERs may have never taken specific courses in the social science research methodology---either quantitative or qualitative---on which their scholarship often relies so heavily. One particular aspect of (quantitative) social science research that differs markedly from disciplines such as biology and chemistry is the instrumentation used to quantify phenomena. In response, this Research Methods essay offers a contemporary social science perspective on test validity and the validation process. The instructional piece explores the concepts of test validity, the validation process, validity evidence, and key threats to validity. The essay also includes an in-depth example of a validity argument and validation approach for a test of student argument analysis. In addition to DBERs, this essay should benefit practitioners (e.g., lab directors, faculty members) in the development, evaluation, and/or selection of instruments for their work assessing students or evaluating pedagogical innovations.},
  pmid = {26903498},
  keywords = {,artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Reeves_Marbach-Ad_2016_Contemporary test validity in theory and practice.pdf}
}

@article{rhemtulla2012,
  title = {When Can Categorical Variables Be Treated as Continuous? {{A}} Comparison of Robust Continuous and Categorical {{SEM}} Estimation Methods under Suboptimal Conditions},
  author = {Rhemtulla, Mijke and {Brosseau-Liard}, Patricia {\'E} and Savalei, Victoria},
  year = {2012},
  month = sep,
  journal = {Psychological Methods},
  volume = {17},
  number = {3},
  pages = {354--373},
  issn = {1082989X},
  doi = {10.1037/a0029315},
  urldate = {2021-03-23},
  abstract = {A simulation study compared the performance of robust normal theory maximum likelihood (ML) and robust categorical least squares (cat-LS) methodology for estimating confirmatory factor analysis models with ordinal variables. Data were generated from 2 models with 2-7 categories, 4 sample sizes, 2 latent distributions, and 5 patterns of category thresholds. Results revealed that factor loadings and robust standard errors were generally most accurately estimated using cat-LS, especially with fewer than 5 categories; however, factor correlations and model fit were assessed equally well with ML. Cat-LS was found to be more sensitive to sample size and to violations of the assumption of normality of the underlying continuous variables. Normal theory ML was found to be more sensitive to asymmetric category thresholds and was especially biased when estimating large factor loadings. Accordingly, we recommend cat-LS for data sets containing variables with fewer than 5 categories and ML when there are 5 or more categories, sample size is small, and category thresholds are approximately symmetric. With 6-7 categories, results were similar across methods for many conditions; in these cases, either method is acceptable. {\copyright} 2012 American Psychological Association.},
  pmid = {22799625},
  keywords = {,artigoCFA,Categorical indicators,Categorical leastsquares,Confirmatory factor analysis,Estimator,Evidencia,Importante,Lido,Likert,Maximum likelihood,OneNote,Ordinal data,Robust statistics,SEM},
  file = {C:\Users\pablo\OneDrive\Zotero\Rhemtulla et al_2012_When can categorical variables be treated as continuous.pdf}
}

@misc{ringle2022,
  title = {{{SmartPLS}} 4},
  author = {Ringle, Christian M. and Wende, Sven and Becker, Jan Michael},
  year = {2022},
  address = {Oststeinbek},
  url = {https://www.smartpls.com},
  urldate = {2021-11-24},
  howpublished = {SmartPLS},
  keywords = {artigoCFA}
}

@article{rios2014,
  title = {Validity Evidence Based on Internal Structure},
  author = {Rios, Joseph and Wells, Craig},
  year = {2014},
  journal = {Psicothema},
  volume = {26},
  number = {1},
  pages = {108--116},
  issn = {02149915},
  doi = {10.7334/psicothema2013.260},
  abstract = {Background: Validity evidence based on the internal structure of an assessment is one of the five forms of validity evidence stipulated in the Standards for Educational and Psychological Testing of the American Educational Research Association, American Psychological Association, and National Council on Measurement in Education. In this paper, we describe the concepts underlying internal structure and the statistical methods for gathering and analyzing internal structure. Method: An in-depth description of the traditional and modern techniques for evaluating the internal structure of an assessment. Results: Validity evidence based on the internal structure of an assessment is necessary for building a validity argument to support the use of a test for a particular purpose. Conclusions: The methods described in this paper provide practitioners with a variety of tools for assessing dimensionality, measurement invariance and reliability for an educational test or other types of assessment. {\copyright} 2014 Psicothema.},
  pmid = {24444738},
  keywords = {artigoCFA,Dimensionality,Measurement invariance,Reliability,Standards,Validity},
  file = {C:\Users\pablo\OneDrive\Zotero\Rios_Wells_2014_Validity evidence based on internal structure.pdf}
}

@article{robitzsch2020,
  title = {Why {{Ordinal Variables Can}} ({{Almost}}) {{Always Be Treated}} as {{Continuous Variables}}: {{Clarifying Assumptions}} of {{Robust Continuous}} and {{Ordinal Factor Analysis Estimation Methods}}},
  author = {Robitzsch, Alexander},
  year = {2020},
  month = oct,
  journal = {Frontiers in Education},
  volume = {5},
  issn = {2504-284X},
  doi = {10.3389/feduc.2020.589965},
  keywords = {artigoCFA,Factor Analysis,Importante,Lido,Likert,OneNote,Ordinal data},
  file = {C:\Users\pablo\OneDrive\Zotero\Robitzsch_2020_Why Ordinal Variables Can (Almost) Always Be Treated as Continuous Variables.pdf}
}

@article{robitzsch2022,
  title = {On the {{Bias}} in {{Confirmatory Factor Analysis When Treating Discrete Variables}} as {{Ordinal Instead}} of {{Continuous}}},
  author = {Robitzsch, Alexander},
  year = {2022},
  month = apr,
  journal = {Axioms},
  volume = {11},
  number = {4},
  publisher = {MDPI},
  issn = {20751680},
  doi = {10.3390/axioms11040162},
  abstract = {Confirmatory factor analysis is some of the most widely used statistical techniques in the social sciences. Frequently, variables (i.e., items) stemming from questionnaires are analyzed. Two competing approaches for estimating confirmatory factor analysis can be distinguished. First, ordinal variables could be treated as in the case of continuous variables using Pearson correlations, and maximum likelihood estimation method would be applied. Second, an ordinal factor analysis based on polychoric correlations can be fitted. In the majority of the psychometric literature, there is a preference for the ordinal factor analysis based on polychoric correlations because the continuous treatment of variables results in biased factor loadings and biased factor correlations. This article argues that it is not legitimate to speak about bias when comparing the two competing factor analytic approaches because it depends on how true model parameters are defined. This decision can be made individually by a researcher. It is shown in simulation studies and analytical derivations that treating variables ordinally using polychoric correlations instead of continuous using Pearson correlations can also lead to biased estimates of factor loadings and factor correlations. Consequently, it should only be stated that different model parameters are defined in a continuous and an ordinal treatment, and one approach should not generally be preferred over the other.},
  keywords = {,artigoCFA,Categorical variables,Confirmatory factor analysis,Estimation methods,Importante,Lido,Likert,OneNote,Ordinal data,Ordinal items},
  file = {C:\Users\pablo\OneDrive\Zotero\Robitzsch_2022_On the Bias in Confirmatory Factor Analysis When Treating Discrete Variables as.pdf}
}

@misc{rogers2021dbharvard,
  title = {Replication {{Data}} for "{{Best Practices}} for {{Your Exploratory Factor Analysis}}: A {{Factor Tutorial}}"},
  author = {Rogers, Pablo},
  year = {2021},
  journal = {RAC-Revista de Administra{\c c}{\~a}o Contempor{\^a}nea},
  publisher = {Harvard Dataverse, V1},
  doi = {10.7910/DVN/RCX8FF},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  keywords = {artigoCFA}
}

@misc{rogers2021dbmendeley,
  title = {Data for "{{Best Practices}} for {{Your Exploratory Factor Analysis}}: A {{Factor Tutorial}}"},
  author = {Rogers, Pablo},
  year = {2021},
  journal = {RAC-Revista de Administra{\c c}{\~a}o Contempor{\^a}nea},
  publisher = {Mendeley Data, V2},
  doi = {10.17632/rdky78bk8r.2},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  keywords = {artigoCFA}
}

@article{rogers2022,
  title = {Best {{Practices}} for {{Your Exploratory Factor Analysis}}: {{A Factor Tutorial}}},
  author = {Rogers, Pablo},
  year = {2022},
  journal = {Revista de Administra{\c c}{\~a}o Contempor{\^a}nea},
  volume = {26},
  number = {6},
  issn = {1982-7849},
  doi = {10.1590/1982-7849rac2022210085.en},
  abstract = {Context: Exploratory factor analysis (EFA) is one of the statistical methods most widely used in administration; however, its current practice coexists with rules of thumb and heuristics given half a century ago. Objective: the purpose of this article is to present the best practices and recent recommendations for a typical EFA in administration through a practical solution accessible to researchers. Methods: in this sense, in addition to discussing current practices versus recommended practices, a tutorial with real data on Factor is illustrated. The Factor software is still little known in the administration area, but is freeware, easy-to-use (point and click), and powerful. The step-by-step tutorial illustrated in the article, in addition to the discussions raised and an additional example, is also available in the format of tutorial videos. Conclusion: through the proposed didactic methodology (article-tutorial + video-tutorial), we encourage researchers/methodologists who have mastered a particular technique to do the same. Specifically about EFA, we hope that the presentation of the Factor software, as a first solution, can transcend the current outdated rules of thumb and heuristics, by making best practices accessible to administration researchers.},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Rogers_2022_Best Practices for Your Exploratory Factor Analysis.pdf}
}

@techreport{rogers2023,
  type = {Preprint},
  title = {Best {{Practices}} for Your {{Confirmatory Factor Analysis}}: {{A JASP}} and Lavaan {{Tutorial}}},
  shorttitle = {Best {{Practices}} for Your {{Confirmatory Factor Analysis}}},
  author = {Rogers, Pablo},
  year = {2023},
  month = nov,
  institution = {Open Science Framework},
  doi = {10.31219/osf.io/57efj},
  url = {https://osf.io/57efj},
  urldate = {2024-01-26},
  abstract = {This article presents a set of guidelines for conducting a typical CFA, drawing from recent empirical research. We provide a practical contribution by introducing and developing a tutorial example within the JASP and lavaan software platforms. Supplementary materials such as videos , files, and scrpits are freely available.},
  copyright = {Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC-BY-NC-SA)},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Rogers_2023_Best Practices for your Confirmatory Factor Analysis.pdf}
}

@article{rosseel2012,
  title = {Lavaan: {{An R}} Package for Structural Equation Modeling},
  author = {Rosseel, Yves},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {48},
  number = {2},
  pages = {1--36},
  publisher = {American Statistical Association},
  doi = {10.18637/jss.v048.i02},
  urldate = {2021-03-23},
  abstract = {Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. Over the years, many software packages for structural equation modeling have been developed, both free and commercial. However, perhaps the best state-of-the-art software packages in this field are still closed- source and/or commercial. The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice.},
  keywords = {,artigoCFA,Factor analysis,Latent variables,lavaan,Path analysis,R packages,R software,Structural equation modeling},
  file = {C:\Users\pablo\OneDrive\Zotero\Rosseel_2012_Lavaan.pdf}
}

@article{schumacker2021,
  title = {Resources for {{Identifying Measurement Instruments}} for {{Social Science Research}}},
  author = {Schumacker, Randall E. and Wind, Stefanie A. and Holmes, Lauren F.},
  year = {2021},
  month = oct,
  journal = {Measurement: Interdisciplinary Research and Perspectives},
  volume = {19},
  number = {4},
  pages = {250--257},
  issn = {1536-6367},
  doi = {10.1080/15366367.2021.1950486},
  keywords = {,artigoCFA,Escalas,Importante,Lido},
  file = {C:\Users\pablo\OneDrive\Zotero\Schumacker et al_2021_Resources for Identifying Measurement Instruments for Social Science Research.pdf}
}

@article{shek2014,
  title = {Use of Structural Equation Modeling in Human Development Research},
  author = {Shek, Daniel T.L. and Yu, Lu},
  year = {2014},
  month = may,
  journal = {International Journal on Disability and Human Development},
  volume = {13},
  number = {2},
  pages = {157--167},
  publisher = {Freund Publishing House Ltd},
  issn = {21910367},
  doi = {10.1515/ijdhd-2014-0302},
  urldate = {2021-03-22},
  abstract = {In social sciences, it is common to hypothesize that latent factors (e.g., psychological well-being) underlie the observed variables (e.g., depression and risk behavior). Hence, it is important to examine the nature of latent variables, the inter-relationships among such variables, and their associations with other predictors and outcome variables. These latent variable-related issues can be well addressed by adopting the approach of structural equation modeling. Apart from describing the use of structural equation modeling in research on human development, this paper also presents the assumptions underlying structural equation modeling, steps of model construction and model assessment, and both the strengths and limitations of this method in human development research. Finally, some examples using structural equation modeling in the Chinese contexts are also illustrated. {\copyright} 2014 by Walter de Gruyter Berlin/Boston.},
  keywords = {,artigoCFA,Geral,human development,Importante,latent variables analyses,research methods,SEM,structural equation modeling},
  file = {C\:\\Users\\pablo\\OneDrive\\Zotero\\Shek_Yu_2014_Use of structural equation modeling in human development research.pdf;C\:\\Users\\pablo\\OneDrive\\Zotero\\Shek_Yu_2014_Use of structural equation modeling in human development research2.pdf}
}

@article{shi2020b,
  title = {The {{Effect}} of {{Estimation Methods}} on {{SEM Fit Indices}}},
  author = {Shi, Dexin and {Maydeu-Olivares}, Alberto},
  year = {2020},
  month = jun,
  journal = {Educational and Psychological Measurement},
  volume = {80},
  number = {3},
  pages = {421--445},
  publisher = {SAGE Publications Inc.},
  issn = {15523888},
  doi = {10.1177/0013164419885164},
  urldate = {2021-04-13},
  abstract = {We examined the effect of estimation methods, maximum likelihood (ML), unweighted least squares (ULS), and diagonally weighted least squares (DWLS), on three population SEM (structural equation modeling) fit indices: the root mean square error of approximation (RMSEA), the comparative fit index (CFI), and the standardized root mean square residual (SRMR). We considered different types and levels of misspecification in factor analysis models: misspecified dimensionality, omitting cross-loadings, and ignoring residual correlations. Estimation methods had substantial impacts on the RMSEA and CFI so that different cutoff values need to be employed for different estimators. In contrast, SRMR is robust to the method used to estimate the model parameters. The same criterion can be applied at the population level when using the SRMR to evaluate model fit, regardless of the choice of estimation method.},
  keywords = {,artigoCFA,CFI,close fit,DWLS,estimation methods,Estimator,Evidencia,fit indices,Importante,Indices,Lido,ML,RMSEA,SEM,SRMR,structural equation modeling (SEM),ULS},
  file = {C:\Users\pablo\OneDrive\Zotero\Shi_Maydeu-Olivares_2020_The Effect of Estimation Methods on SEM Fit Indices.pdf}
}

@article{shi2020mi,
  title = {Fitting {{Ordinal Factor Analysis Models With Missing Data}}: {{A Comparison Between Pairwise Deletion}} and {{Multiple Imputation}}},
  shorttitle = {Fitting {{Ordinal Factor Analysis Models With Missing Data}}},
  author = {Shi, Dexin and Lee, Taehun and Fairchild, Amanda J. and {Maydeu-Olivares}, Alberto},
  year = {2020},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {80},
  number = {1},
  pages = {41--66},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/0013164419845039},
  url = {http://journals.sagepub.com/doi/10.1177/0013164419845039},
  urldate = {2024-01-30},
  abstract = {This study compares two missing data procedures in the context of ordinal factor analysis models: pairwise deletion (PD; the default setting in Mplus) and multiple imputation (MI). We examine which procedure demonstrates parameter estimates and model fit indices closer to those of complete data. The performance of PD and MI are compared under a wide range of conditions, including number of response categories, sample size, percent of missingness, and degree of model misfit. Results indicate that both PD and MI yield parameter estimates similar to those from analysis of complete data under conditions where the data are missing completely at random (MCAR). When the data are missing at random (MAR), PD parameter estimates are shown to be severely biased across parameter combinations in the study. When the percentage of missingness is less than 50\%, MI yields parameter estimates that are similar to results from complete data. However, the fit indices (i.e., {$\chi$}               2               , RMSEA, and WRMR) yield estimates that suggested a worse fit than results observed in complete data. We recommend that applied researchers use MI when fitting ordinal factor models with missing data. We further recommend interpreting model fit based on the TLI and CFI incremental fit indices.},
  langid = {english},
  keywords = {artigoCFA,Evidencia,Missing Data,Ordinal data},
  file = {C\:\\Users\\pablo\\OneDrive\\Zotero\\Shi et al_2020_Fitting Ordinal Factor Analysis Models With Missing Data.pdf;C\:\\Users\\pablo\\Zotero\\storage\\QUKDLCCE\\Shi et al. - 2020 - Fitting Ordinal Factor Analysis Models With Missin.pdf}
}

@article{shi2020srmr,
  title = {Assessing {{Fit}} in {{Ordinal Factor Analysis Models}}: {{SRMR}} vs. {{RMSEA}}},
  author = {Shi, Dexin and {Maydeu-Olivares}, Alberto and Rosseel, Yves},
  year = {2020},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {1},
  pages = {1--15},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2019.1611434},
  url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2019.1611434},
  abstract = {This study introduces the statistical theory of using the Standardized Root Mean Squared Error (SRMR) to test close fit in ordinal factor analysis. We also compare the accuracy of confidence intervals (CIs) and tests of close fit based on the SRMR with those obtained based on the Root Mean Squared Error of Approximation (RMSEA). The current (biased) implementation for the RMSEA never rejects that a model fits closely when data are binary and almost invariably rejects the model in large samples if data consist of five categories. The unbiased RMSEA produces better rejection rates, but it is only accurate enough when the number of variables is small and the degree of misfit is small. In contrast, across all simulated conditions, the tests of close fit based on the SRMR yield acceptable type I error rates. SRMR tests of close fit are also more powerful than those using the unbiased RMSEA.},
  keywords = {,artigoCFA,close fit,Evidencia,Indices,Ordinal data,Ordinal factor analysis,Para LER,RMSEA,SRMR},
  file = {C:\Users\pablo\OneDrive\Zotero\Shi et al_2020_Assessing Fit in Ordinal Factor Analysis Models.pdf}
}

@incollection{sireci2013,
  title = {Test Validity},
  booktitle = {{{APA}} Handbook of Testing and Assessment in Psychology, {{Vol}}. 1: {{Test}} Theory and Testing and Assessment in Industrial and Organizational Psychology.},
  author = {Sireci, Stephen G. and Sukin, Tia},
  year = {2013},
  pages = {61--84},
  publisher = {American Psychological Association},
  address = {Washington},
  doi = {10.1037/14047-004},
  url = {http://content.apa.org/books/14047-004},
  keywords = {,artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Sireci_Sukin_2013_Test validity.pdf}
}

@misc{soper2024,
  title = {A-Priori {{Sample Size Calculator}} for {{Structural Equation Models}}},
  author = {Soper, Daniel S.},
  year = {2024},
  url = {https://www.danielsoper.com/statcalc},
  urldate = {2021-10-08},
  howpublished = {[Software]},
  keywords = {artigoCFA}
}

@article{trizano-hermosilla2016,
  title = {Best Alternatives to {{Cronbach}}'s Alpha Reliability in Realistic Conditions: {{Congeneric}} and Asymmetrical Measurements},
  author = {{Trizano-Hermosilla}, Italo and Alvarado, Jes{\'u}s M.},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  number = {MAY},
  publisher = {Frontiers Media S.A.},
  issn = {16641078},
  doi = {10.3389/fpsyg.2016.00769},
  urldate = {2021-04-05},
  abstract = {The Cronbach's alpha is the most widely used method for estimating internal consistency reliability. This procedure has proved very resistant to the passage of time, even if its limitations are well documented and although there are better options as omega coefficient or the different versions of glb, with obvious advantages especially for applied research in which the {\'i}tems differ in quality or have skewed distributions. In this paper, using Monte Carlo simulation, the performance of these reliability coefficients under a one-dimensional model is evaluated in terms of skewness and no tau-equivalence. The results show that omega coefficient is always better choice than alpha and in the presence of skew items is preferable to use omega and glb coefficients even in small samples.},
  keywords = {,Alpha,artigoCFA,Asymmetrical measures,Evidencia,GLB,Greatest lower bound,Lido,Omega,OneNote,Reliability},
  file = {C:\Users\pablo\OneDrive\Zotero\Trizano-Hermosilla_Alvarado_2016_Best alternatives to Cronbach's alpha reliability in realistic conditions.pdf}
}

@book{urbina2014,
  title = {Essentials of {{Psychological Testing}}},
  author = {Urbina, Susana},
  year = {2014},
  publisher = {John Wiley \& Sons},
  address = {Hoboken, New Jersey},
  isbn = {978-1-118-70725-8},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Urbina_2014_Essentials of Psychological Testing.pdf}
}

@article{vanlissa2021,
  title = {{{WORCS}}: {{A}} Workflow for Open Reproducible Code in Science},
  shorttitle = {{{WORCS}}},
  author = {Van Lissa, Caspar J. and Brandmaier, Andreas M. and Brinkman, Loek and Lamprecht, Anna-Lena and Peikert, Aaron and Struiksma, Marijn E. and Vreede, Barbara M.I.},
  editor = {Kuhn, Tobias},
  year = {2021},
  month = may,
  journal = {Data Science},
  volume = {4},
  number = {1},
  pages = {29--49},
  issn = {24518492, 24518484},
  doi = {10.3233/DS-210031},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/DS-210031},
  urldate = {2024-01-27},
  abstract = {Adopting open science principles can be challenging, requiring conceptual education and training in the use of new tools. This paper introduces the Workflow for Open Reproducible Code in Science (WORCS): A step-by-step procedure that researchers can follow to make a research project open and reproducible. This workflow intends to lower the threshold for adoption of open science principles. It is based on established best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals, institutions, and funding bodies. To facilitate widespread adoption, the WORCS principles have been implemented in the R package worcs, which offers an RStudio project template and utility functions for specific workflow steps. This paper introduces the conceptual workflow, discusses how it meets different standards for open science, and addresses the functionality provided by the R implementation, worcs. This paper is primarily targeted towards scholars conducting research projects in R, conducting research that involves academic prose, analysis code, and tabular data. However, the workflow is flexible enough to accommodate other scenarios, and offers a starting point for customized solutions. The source code for the R package and manuscript, and a list of examplesof WORCS projects, are available at https://github.com/cjvanlissa/worcs.},
  keywords = {artigoCFA,Figuras,Importante,Lido,OneNote,R workflow},
  file = {C\:\\Users\\pablo\\OneDrive\\Zotero\\Van Lissa et al_2021_WORCS.pdf;C\:\\Users\\pablo\\Zotero\\storage\\HAQE56S9\\Van Lissa et al. - 2021 - WORCS A workflow for open reproducible code in sc.pdf}
}

@article{wang2021,
  title = {Power {{Analysis}} for {{Parameter Estimation}}  in {{Structural Equation Modeling}}:  {{A Discussion}} and {{Tutorial}}},
  author = {Wang, Y. Andre and Rhemtulla, Mijke},
  year = {2021},
  journal = {Advances in Methods and  Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {1--17},
  doi = {10.1177/2515245920918253},
  url = {https://doi.org/10.1177/2515245920918253},
  keywords = {artigoCFA,Lido,OneNote,Power analysis,R packages,Simulation},
  file = {C:\Users\pablo\OneDrive\Zotero\Wang_Rhemtulla_2021_Power Analysis for Parameter Estimation in Structural Equation Modeling.pdf}
}

@techreport{wang2023,
  type = {Preprint},
  title = {How to {{Conduct Power Analysis}} for {{Structural Equation Models}}: {{A Practical Primer}}},
  shorttitle = {How to {{Conduct Power Analysis}} for {{Structural Equation Models}}},
  author = {Wang, Yilin Andre},
  year = {2023},
  month = may,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/4n3uk},
  url = {https://osf.io/4n3uk},
  urldate = {2024-02-16},
  abstract = {Structural equation modeling (SEM) is popular, but planning for studies that use SEM for data analysis can be difficult. As power analysis becomes standard practice in many fields of psychology, researchers who use SEM for data analysis can benefit from knowing how to conduct power analysis for their studies. With this article, I offer a gentle, practical introduction to power analysis for SEM. First, I connect two goals that researchers often have when using SEM---to interpret the overall model and to detect target effects within the model---to power analysis. Then, I conceptually describe power to detect target effects and power to detect model misfit, summarizing what determines each and common approaches to conducting each type of power analysis. Finally, I provide an illustrative example of conducting power analysis for SEM with a concrete research scenario. Throughout the article, I prioritize plain language and practical guidance over technical depth, with the hope that it makes power analysis for SEM less daunting.},
  keywords = {artigoCFA,Importante,Lido,OneNote,Power analysis},
  file = {C:\Users\pablo\OneDrive\Zotero\Wang_2023_How to Conduct Power Analysis for Structural Equation Models.pdf}
}

@incollection{west2023,
  title = {Model {{Fit}} in {{Structural Equation Modeling}}},
  booktitle = {Handbook of {{Structural Equation Modeling}}},
  author = {West, Stephen G. and Wu, Wei and McNeish, Daniel and Savord, Andrea},
  editor = {Hoyle, Rick H.},
  year = {2023},
  publisher = {The Guilford Press},
  address = {New York},
  keywords = {artigoCFA,Importante,Indices,Lido,OneNote,SEM},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Hoyle (2023) - Handbook of Structural Equation Modeling.pdf}
}

@article{westland2010,
  title = {Lower Bounds on Sample Size in Structural Equation Modeling},
  author = {Westland, Christopher J.},
  year = {2010},
  month = nov,
  journal = {Electronic Commerce Research and Applications},
  volume = {9},
  number = {6},
  issn = {15674223},
  doi = {10.1016/j.elerap.2010.07.003},
  keywords = {artigoCFA,Power analysis,SEM},
  file = {C:\Users\pablo\OneDrive\Zotero\Christopher Westland_2010_Lower bounds on sample size in structural equation modeling.pdf}
}

@book{whittaker2022,
  title = {A {{Beginner}}'s {{Guide}} to {{Structural Equation Modeling}}},
  author = {Whittaker, Tiffany A. and Schumacker, Randall E.},
  year = {2022},
  edition = {Fifth Edition},
  publisher = {Routledge Taylor \& Francis Group},
  address = {New York},
  keywords = {artigoCFA},
  file = {C:\Users\pablo\OneDrive\Books\CB-SEM\Main\Whittaker and Schumacker (2022) - A Beginner's Guide to Structural Equation Modeling.pdf}
}

@article{widodo2018,
  title = {Some {{Notes}} on the {{Contemporary Views}} of {{Validity}} in {{Psychological}} and {{Educational Assessment}}},
  author = {Widodo, Estu},
  year = {2018},
  journal = {Advances in Social Science, Education and Humanities Research},
  volume = {231},
  pages = {732--734},
  doi = {10.3968/8877},
  url = {https://doi.org/10.1111/emip.12052,},
  abstract = {This paper aims at providing an analysis of views of validity as an important concept in psychological and educational assessment which has undergone transformation for several decades. The revolutionary change took place along with the release of the 1999 Standards followed by the 2014 Standards in which validity is associated with interpretation and use of test scores. With the new framework, several types of theoretical and empirical evidence are required to support the validation process. The results of the analysis show that the drastic change of the validity concept and the complicated process of validation have resulted in misunderstanding, misinterpretation, and misuse. Apart from different views which challenge the new concept, the argument-based approach has provided basis for new studies in many fields. Some strategies required to deal with complicated validation processes are suggested, along with some alternative perspective associated with the limitations of the standards. Keywords-validity, assessment, use of test score, validity evidence, interpretation, argument-based validity.},
  keywords = {,artigoCFA},
  file = {C:\Users\pablo\OneDrive\Zotero\Widodo_2018_Some Notes on the Contemporary Views of Validity in Psychological and.pdf}
}

@article{wilkinson2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and Da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and 'T Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and Van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and Van Der Lei, Johan and Van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2024-01-26},
  abstract = {Abstract             There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders---representing academia, industry, funding agencies, and scholarly publishers---have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {artigoCFA,Geral},
  file = {C:\Users\pablo\OneDrive\Zotero\Wilkinson et al_2016_The FAIR Guiding Principles for scientific data management and stewardship.pdf}
}

@article{wolfmcneish2023R,
  title = {Dynamic : {{An R Package}} for {{Deriving Dynamic Fit Index Cutoffs}} for {{Factor Analysis}}},
  author = {Wolf, Melissa G. and McNeish, Daniel},
  year = {2023},
  month = jan,
  journal = {Multivariate Behavioral Research},
  volume = {58},
  number = {1},
  pages = {189--194},
  issn = {0027-3171},
  doi = {10.1080/00273171.2022.2163476},
  keywords = {artigoCFA,DFI,Importante,Indices,Para LER,R packages},
  file = {C:\Users\pablo\OneDrive\Zotero\Wolf_McNeish_2023_dynamic.pdf}
}

@article{xia2018,
  title = {The {{Influence}} of {{Number}} of {{Categories}} and {{Threshold Values}} on {{Fit Indices}} in {{Structural Equation Modeling}} with {{Ordered Categorical Data}}},
  author = {Xia, Yan and Yang, Yanyun},
  year = {2018},
  month = sep,
  journal = {Multivariate Behavioral Research},
  volume = {53},
  number = {5},
  pages = {731--755},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2018.1480346},
  url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2018.1480346},
  urldate = {2024-02-01},
  langid = {english},
  keywords = {artigoCFA,DWLS,Estimator,Evidencia,Importante,Indices,Ordinal data,Para LER,SEM,ULS},
  file = {C:\Users\pablo\OneDrive\Zotero\Xia_Yang_2018_The Influence of Number of Categories and Threshold Values on Fit Indices in.pdf}
}

@article{xia2019,
  title = {{{RMSEA}}, {{CFI}}, and {{TLI}} in Structural Equation Modeling with Ordered Categorical Data: {{The}} Story They Tell Depends on the Estimation Methods},
  author = {Xia, Yan and Yang, Yanyun},
  year = {2019},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {1},
  pages = {409--428},
  publisher = {Springer New York LLC},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-1055-2},
  url = {http://link.springer.com/10.3758/s13428-018-1055-2},
  urldate = {2021-04-13},
  abstract = {In structural equation modeling, application of the root mean square error of approximation (RMSEA), comparative fit index (CFI), and Tucker--Lewis index (TLI) highly relies on the conventional cutoff values developed under normal-theory maximum likelihood (ML) with continuous data. For ordered categorical data, unweighted least squares (ULS) and diagonally weighted least squares (DWLS) based on polychoric correlation matrices have been recommended in previous studies. Although no clear suggestions exist regarding the application of these fit indices when analyzing ordered categorical variables, practitioners are still tempted to adopt the conventional cutoff rules. The purpose of our research was to answer the question: Given a population polychoric correlation matrix and a hypothesized model, if ML results in a specific RMSEA value (e.g.,.08), what is the RMSEA value when ULS or DWLS is applied? CFI and TLI were investigated in the same fashion. Both simulated and empirical polychoric correlation matrices with various degrees of model misspecification were employed to address the above question. The results showed that DWLS and ULS lead to smaller RMSEA and larger CFI and TLI values than does ML for all manipulated conditions, regardless of whether or not the indices are scaled. Applying the conventional cutoffs to DWLS and ULS, therefore, has a pronounced tendency not to discover model--data misfit. Discussions regarding the use of RMSEA, CFI, and TLI for ordered categorical data are given.},
  keywords = {,artigoCFA,CFI,Diagonally weighted least squares,DWLS,Estimator,Fit index,Importante,Indices,Lido,Maximum likelihood,ML,Ordered categorical data,Ordinal data,RMSEA,SEM,Structural equation modeling,TLI,ULS,Unweighted least squares},
  file = {C:\Users\pablo\OneDrive\Zotero\Xia_Yang_2019_RMSEA, CFI, and TLI in structural equation modeling with ordered categorical.pdf}
}

@article{yang-wallentin2010,
  title = {Confirmatory Factor Analysis of Ordinal Variables with Misspecified Models},
  author = {{Yang-Wallentin}, Fan and J{\"o}reskog, Karl G. and Luo, Hao},
  year = {2010},
  journal = {Structural Equation Modeling},
  volume = {17},
  number = {3},
  pages = {392--423},
  issn = {10705511},
  doi = {10.1080/10705511.2010.489003},
  urldate = {2021-03-22},
  abstract = {Ordinal variables are common in many empirical investigations in the social and behavioral sciences. Researchers often apply the maximum likelihood method to fit structural equation models to ordinal data. This assumes that the observed measures have normal distributions, which is not the case when the variables are ordinal. A better approach is to use polychoric correlations and fit the models using methods such as unweighted least squares (ULS), maximum likelihood (ML), weighted least squares (WLS), or diagonally weighted least squares (DWLS). In this simulation evaluation we study the behavior of these methods in combination with polychoric correlations when the models are misspecified. We also study the effect of model size and number of categories on the parameter estimates, their standard errors, and the common chi-square measures of fit when the models are both correct and misspecified. When used routinely, these methods give consistent parameter estimates but ULS, ML, and DWLS give incorrect standard errors. Correct standard errors can be obtained for these methods by robustification using an estimate of the asymptotic covariance matrix W of the polychoric correlations. When used in this way the methods are here called RULS, RML, and RDWLS. {\copyright} Taylor \& Francis Group, LLC.},
  keywords = {,artigoCFA,DWLS,Estimator,Evidencia,ML,Para LER,RDWLS,RML,Robust estimator,RULS,ULS,WLS},
  file = {C:\Users\pablo\OneDrive\Zotero\Yang-Wallentin et al_2010_Confirmatory factor analysis of ordinal variables with misspecified models.pdf}
}

@article{yang2013,
  title = {Confirmatory Factor Analysis under Violations of Distributional and Structural Assumptions},
  author = {Yang, Yanyun and Liang, Xinya},
  year = {2013},
  journal = {Int. J. Quantitative Research in Education},
  volume = {1},
  number = {1},
  pages = {61--84},
  abstract = {This simulation study evaluated CFA model results under violations of both distributional and structural assumptions using maximum likelihood (ML), robust maximum likelihood (RML), and weighted least square (WLS) estimation methods. Design factors included model complexity, the degree of non-normality of factor and error scores, sample sizes, and model misspecifications. In total, 72 conditions were used for data generation. Results were evaluated by rejection rate based on the model chi-square tests, fit function, CFI, RMSEA, and parameter estimates. Findings from the simulation study suggested that CFA results were robust to moderate violation of the non-normality of both factor and error scores under ML and RML methods, however, the degree of non-normality of factor scores impacted both overall model fit indices and loading estimates under WLS, particularly when the models were mis-specified. In addition, correctly specified and mis-specified models were likely detected by combining results from multiple estimation methods. Reference to this paper should be made as follows: Yang, Y. and Liang, X. (2013) 'Confirmatory factor analysis under violations of distributional and structural assumptions', Int.},
  keywords = {,artigoCFA,CFA,confirmatory factor analysis,distributional assumption,Estimator,Evidencia,ML,RML,structural assumption,WLS},
  file = {C:\Users\pablo\OneDrive\Zotero\Yang_Liang_2013_Confirmatory factor analysis under violations of distributional and structural.pdf}
}

@article{yuan2016,
  title = {Assessing {{Structural Equation Models}} by {{Equivalence Testing With Adjusted Fit Indexes}}},
  author = {Yuan, Ke Hai and Chan, Wai and Marcoulides, George A. and Bentler, Peter M.},
  year = {2016},
  month = may,
  journal = {Structural Equation Modeling},
  volume = {23},
  number = {3},
  pages = {319--330},
  publisher = {Routledge},
  issn = {15328007},
  doi = {10.1080/10705511.2015.1065414},
  abstract = {Conventional null hypothesis testing (NHT) is a very important tool if the ultimate goal is to find a difference or to reject a model. However, the purpose of structural equation modeling (SEM) is to identify a model and use it to account for the relationship among substantive variables. With the setup of NHT, a nonsignificant test statistic does not necessarily imply that the model is correctly specified or the size of misspecification is properly controlled. To overcome this problem, this article proposes to replace NHT by equivalence testing, the goal of which is to endorse a model under a null hypothesis rather than to reject it. Differences and similarities between equivalence testing and NHT are discussed, and new ``T-size'' terminology is introduced to convey the goodness of the current model under equivalence testing. Adjusted cutoff values of root mean square error of approximation (RMSEA) and comparative fit index (CFI) corresponding to those conventionally used in the literature are obtained to facilitate the understanding of T-size RMSEA and CFI. The single most notable property of equivalence testing is that it allows a researcher to confidently claim that the size of misspecification in the current model is below the T-size RMSEA or CFI, which gives SEM a desirable property to be a scientific methodology. R code for conducting equivalence testing is provided in an appendix.},
  keywords = {,artigoCFA,CFI,EQT,Importante,Indices,Lido,likelihood ratio statistic,OneNote,RMSEA,tolerance size,Type I error},
  file = {C:\Users\pablo\OneDrive\Zotero\Yuan et al_2016_Assessing Structural Equation Models by Equivalence Testing With Adjusted Fit.pdf}
}

@book{zhang2018,
  title = {Practical Statistical Power Analysis Using {{Webpower}} and {{R}}},
  author = {Zhang, Zhiyong and Yuan, Ke-Hai},
  year = {2018},
  publisher = {ISDSA Press},
  address = {Indiana},
  url = {https://doi.org/10.35566/power},
  urldate = {2023-04-13},
  keywords = {artigoCFA,Importante,Power analysis,R packages,R software,Simulation},
  file = {C:\Users\pablo\OneDrive\Documentos\Temas\CB-SEM\Subtemas\C√°lculo de amostra\Zhang e Yuan 2018.pdf}
}
